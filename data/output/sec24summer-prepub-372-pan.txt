# Is It a Trap? A Large-scale Empirical Study And Comprehensive Assessment of Online Automated Privacy Policy Generators for Mobile Apps
# Shidong Pan∗ Dawen Zhang Mark Staples
# CSIRO’s Data61 & ANU CSIRO’s Data61 & ANU CSIRO’s Data61
# Zhenchang Xing Jieshan Chen Xiwei Xu Thong Hoang†
# CSIRO’s Data61 & ANU CSIRO’s Data61 CSIRO’s Data61 CSIRO’s Data61
# Abstract
Privacy regulations protect and promote the privacy of individuals by requiring mobile apps to provide a privacy policy that explains what personal information is collected and how these apps process this information. However, developers often do not have sufficient legal knowledge to create such privacy policies. Online Automated Privacy Policy Generators (APPGs) can create privacy policies, but their quality and other characteristics can vary. In this paper, we conduct the first large-scale empirical study and comprehensive assessment of APPGs for mobile apps. Specifically, we scrutinize 10 APPGs on multiple dimensions. We further perform the market penetration analysis by collecting 46,472 Android app privacy policies from Google Play, discovering that nearly 20% of privacy policies could be generated by existing APPGs. Lastly, we point out that generated policies in our study do not fully comply with GDPR, CCPA, or LGPD. In summary, app developers must carefully select and use the appropriate APPGs with careful consideration to avoid potential pitfalls.
# 1 Introduction
Mobile phones and apps are now a ubiquitous part of digital life. There is a large variety and volume of data collected and used by mobile apps, which inevitably brings many privacy issues . Privacy policies inform users about what, why, and how their personal data are collected and used. These privacy policies have become an important element of responsible technology in mobile app ecosystems. They also form part of legal agreements for apps and services. Specifically, they are required under regulation in many jurisdictions, such as European General Data Protection Regulation (GDPR) , California Consumer Privacy Act (CCPA) , Brazilian Law of General Data Protection (LGPD) , Australian Privacy Principles (APP) , and Chinese Personal Information Protection Law (PIPL) . For example, APP [Art. 1, §1] imposes an obligation on organisational entities to “have a clearly expressed and up-to-date APP Privacy Policy about how the entity manages personal information.” Privacy policies must be consistent with the data collected and functions provided by service providers, and inadequate policies can create significant business and legal problems.
Developing privacy policies is a complex process, demanding both the knowledge of app features and corresponding legal requirements. While larger companies have greater resources and legal expertise to create high-quality privacy policies for their apps, most (citizen) developers do not have legal support and struggle to prepare accurate privacy policies . To develop privacy policies, developers may copy-paste-modify existing privacy policies, ad-hoc. As part of this big picture, app development by non-professional developers is growing quickly, supported by trends of using no-code/low-code automated app development tools  and pre-trained Large Language Models (LLMs). To address these needs, Online Automated Privacy Policy Generators (APPGs) can provide more automated solutions and more systematic support for developers to create privacy policies for their apps, rather than through ad-hoc reuse. Figure 1 provides several illustrative examples. Most APPGs are questionnaire-based tools, which work by asking app developers a series of privacy-related questions about the app, and using those answers to generate privacy policies. APPGs can create privacy policies, but their quality and other characteristics can vary and are not yet deeply understood.
As we observe in this paper, many apps fail to provide a privacy policy, perfunctorily provide only a low-quality privacy policy, or provide a privacy policy not in local language. APPGs are becoming an increasingly popular solution used by developers, but developers can be unaware of hidden issues in APPGs . Potential design flaws may reflect in the policies, amplify vulnerabilities, violate users’ trust assumptions, and ultimately harm end-users. Thus, a comprehensive and systematic assessment of the capability and limitations of APPGs is necessary. To better understand the scale of potential issues...
# App Name
Please provide an App Name!
# 1. Contact Information
Email Address
# Personally Identifiable Information
Personally Identifiable Information you collect (comma separated)
# App Type
Select app type
# Mobile OS
Select mobile OS
# Policy Effective Date
05/06/2023
# Owner Type
Select owner type
# Examples of APPGs
(a) #1 Iubenda, UI-mode
(b) #2 App Privacy Policy Generator, questionnaire-mode
(c) #3 Termly, questionnaire-mode
(d) #10 Lawpath, document-mode
Potential problems and their broader impact, it is worthwhile to conduct a market penetration analysis on mainstream APPGs. Moreover, by scrutinizing the features of popular APPGs, we can glean insights into the demands and preferences of both the market and developers.
Numerous previous studies have analysed privacy policies  from various perspectives. However, the majority of existing compliance analyses in relation to privacy regulations lack fine-grained scrutiny of specific clauses and requirements. APPGs often claim the generated policies are compliant with privacy regulations. A more nuanced analysis that hones in on the specific requirements stipulated by privacy regulations is important for a deeper understanding of APPGs’ quality. Other issues relate to mobile apps’ dependence on device permissions, for instance, LOCATION and CAMERA, to function normally. Generally, we find users are more vigilant when directly providing personal information, but underestimate the impact brought by device permission requests, which when granted by a user allow apps to continuously collect critical personal information without further user approval or consent. Therefore, it is significant to examine whether developers correctly display the needed device permissions in the generated privacy policies with APPGs for mobile apps.
# Research Questions
To evaluate current APPGs and their use, we investigate the following research questions:
- RQ1: What APPGs exist for mobile apps? What are the differences between them? (Section 2)
- RQ2: How many mobile apps’ privacy policies could be generated by APPGs? Why are some APPGs more popular? (Section 3)
- RQ3: To what extent do APPGs comply with privacy regulations in terms of specific requirements, data rights, device permission disclosure, and self-integrity? (Section 4)
An overview of our methodology is shown in Figure 2. We first collected an initial set of APPGs. After removing duplicates and filtering out irrelevant results, we identified 10 APPGs for mobile apps as our research objects. We conducted an assessment on multiple dimensions, including their features and the recognition of the extent of data use. We then conducted a large-scale market penetration analysis. Based on a random sample of apps from the Google Play app store, we successfully downloaded 46,472 privacy policies (Crawled Privacy Policy Collection) as a large-scale dataset for mobile apps. We found that 15% of apps do not provide a privacy policy link, 22% of them are low-quality privacy policies, and 20% of these privacy policies fail to provide a privacy.
# Observed issues
# RQ1: APPGs evaluation
- Missing and unavailable privacy policies
- Low quality privacy policies
- Language non-localization
- Extent of data use
# Preprocess apps data
# Apps dataset
- Crawl apps from Google Play Store
- Remove apps without a privacy policy link
- Download privacy policies
# Privacy Policy Collection
# RQ2: Market penetration
- Fingerprint Keyword Searching
- Document Similarity Analysis
# RQ3: Privacy policy analysis
- Compliance against regulations
- Permission coverage analysis
- Contradiction analysis
# APPGs dataset
- Select APPGs applicable for mobile apps
- Design 3 synthetic apps
- Generate privacy policies for synthetic apps
policy in English, the first language in the target market. We then generated 30 privacy policies (Generated Privacy Policy Collection) using the 10 APPGs for three synthetic apps and conducted a market penetration analysis. Our results show 20% app developers favorably use APPGs to generate their privacy policies, and #2 App Privacy Policy Generator is the most popular one, boasting a 72% adoption rate. In an effort to more accurately assess compliance with privacy regulations, we scrutinized the generated privacy policies. Our findings revealed a substantial level of noncompliance with privacy laws and a frequent under-claiming of data rights and highly concerning privacy practices, especially the most popular #2. In addition, our results suggest dangerous permissions are commonly missing in the generated policies of APPGs, and inappropriate APPGs selection will hinder developers’ capacity to include essential device permissions. Furthermore, we found that APPGs might introduce more privacy policy contradiction issues, undermining the self-integrity.
# Contributions
To our knowledge, this is the first large-scale, exhaustive empirical study of automated privacy policy generators for mobile apps. Our observations, findings, and insights will benefit stakeholders beyond just app developers, including APPG providers and privacy regulators from the APPG design perspective and legal perspective. We make the following contributions:
- We perform a systematic empirical analysis of Automated Privacy Policy Generators (APPGs), covering various aspects, such as features, characterizations, and levels of recognition of data use.
- We conduct a systematic analysis of the privacy policies of 99,149 apps in the Google Play Store. Specifically, our dataset includes 46,472 privacy policies extracted from these apps and available at .
- We discuss implications for APPG users, APPG providers, and privacy regulators to improve the APPG ecosystem.
# Online Automated Privacy Policy Generators for Mobile Apps (RQ1)
This section addresses RQ1, describing what APPGs exist for mobile apps and what differences there are between them. We manually collected and identified 10 publicly available online APPGs for mobile apps. We analysed various characteristics and specifically the range of possible data uses in apps.
# Collecting Online APPGs for Mobile Apps
To better understand the current status of APPGs, we identified popular APPGs for mobile apps by using the Google search engine and exploring crowd-knowledge platforms. In detail, we employed the Google search engine to broadly search APPGs for mobile apps. We acted as a hypothetical citizen developer who is trying to generate a privacy policy for their app. We created an initial set of search terms, including “Apps Privacy Policy Generators”, “Apps Privacy Policy Generators Online”, “Privacy Policy Automated Generation”, “Google Play Store Privacy Policy Generator”, “GDPR Policy Generators”. For each search keyword, we evaluated all entries on the first result page. In addition to direct results from Google search, we also identified APPGs by using our Google search terms again in app developer crowd-knowledge platforms including Stack Overflow , GeeksforGeeks , and GitHub . To evaluate search results, we manually identified and read highly related threads, and collected the mentioned APPGs. After collecting an initial set of APPGs and removing duplicates, we filtered out irrelevant APPGs based on the following two criteria: 1) APPGs that only support privacy policy generation for websites; and 2) APPGs that do not actually provide their proposed functions. For example, while some APPGs claim that they are able to generate privacy policies for mobile apps, they do not actually provide such a service.
“Registration” indicates whether a user needs to register an account to use the APPG. All APPGs we collected are based on question answering and template completion (‘boilerplate’), however, they have various different “Modes” to handle information interactions with users as we demonstrated in Figure 1. “User interface”(UI) mode means that users need to select the relative data practices through an integrated UI as à la carte manner, then answer a few other questions about the app’s and developers’ basic information. “Questionnaire” mode only requires users to complete a long questionnaire that covers all potential data practices and a privacy policy will be generated depending on the answers. This is the most common and popular type of APPGs (8/10).
“Document” mode is common in legal consultation websites. Users need to purchase access to a privacy policy document, then answer a series of questions to complete the template. Notably, all analyses of APPGs in this paper include features in both free and paid versions. We conducted all analyses of APPGs in May 2022.
# 2 Characterization
A comprehensive assessment is conducted on identified APPGs along 10 dimensions. The first five (1-5) target practicability, user-friendliness, and convenience from potential APPG users’ perspectives. The next three (6-8) cover legal compliance with privacy regulations, according to the tools’ claims. The last two (9, 10) focus on the understandability of privacy policies. The results are shown in Table 2. We define these 10 dimensions as follows:
1. Extent of Data Recognised (1). Mobile apps normally require data from direct user input or device sensors. Some apps may also provide data to third-party services, for revenue or to enhance the user experience. Privacy regulations such as GDPR [Art. 14(1)(d)] and CCPA [§1798(a)(5)(B), 1798(c), Regs §999(c)(1)(d)))], can require that app developers include information about all collected or shared data within their privacy policy. However, some APPGs only recognise a limited range of data practices for inclusion in privacy policies. As failing to include all data practices could be regarded as a regulatory violation in some jurisdictions, this is an important characteristic and is discussed in more detail in the next section (Section 2). Tentatively, we use ( ) to denote that the APPG recognises a wide range of data use, (G) for a smaller range of data use, and (#) for a few kinds of data use.
2. Customizability (2). Customizability addresses two concerns: can users add privacy practices that were not originally included in the APPG pipeline, and do users have the freedom to add customized clauses in the generated privacy policy document? The former can help developers to provide a more specific and accurate privacy policy; the latter can more easily accommodate developers who want to address additional concerns about users’ privacy. APPGs are denoted as ( ), (G), and (#), as indications that they are positive for both, one, and zero of these questions, respectively.
3. User Instruction (3). The main target audience of APPGs is expected to be developers with little or no legal knowledge about privacy and data protection. Therefore it is crucial to provide sufficient user instructions to help correctly utilize the APPG. User instructions can be generally categorised into three types: providing further explanation of obscure questions (an example is shown in Figure 1), elaborating terms by listing examples, and supporting users with an interactive help center. APPGs are scored as (#) if they only provide one or nil occurrence of the above user instructions, (G) for two to five occurrences, and ( ) for more than five occurrences. Especially, APPG #1 Iubenda also provides an introduction video and comprehensive documentation.
4. Complexity (4). This dimension indicates the general learning cost for APPG users, and higher complexity reflects a higher cost. For UI-mode APPGs, users have to spend more time getting familiar with the UI and learning operation procedures, therefore, we manually set them as ( ). For questionnaire-mode APPGs, we found that the complexity is related to the number of questions that users are asked in the generation process. Based on the “Statistic summary” in Table 3, if the sum of maximum multiple-choice questions and maximum completion is less than 10, the APPG is marked as (#), if it is between 10 to 50, the APPG is marked as (G); and if it is greater than 50, the APPG is marked as ( ). For document-mode APPGs, users only need to select a document from the library and fill up several questions about their basic information, thus, we manually set them as (#).
5. Publishing Support (5). This dimension reflects how convenient it is to deploy the generated privacy policy. Every APPG provides at least one of the following publishing options: 1) a permanently hosted website link containing the privacy policy; and 2) the generated privacy policy context in editable HTML format. In section 3, we presented that a large amount of privacy policy links in the market lead to inaccessible websites, and we believe that a permanently hosted website link provided by APPGs can mitigate this.
#: low level / does not support; G: intermediate level / partially support; : high level / fully support
These three dimensions are binary, simply indicating whether the APPG claims that they provide compliance with the corresponding privacy regulations. ( ) denotes support and (#) denotes non-support. No APPG claims compliance with other privacy regulations such as APP  and PIPL .
Multilingual Support (9). In Section 3, we discuss how current privacy policies suffer from the language localization problems. APPGs that provide multi-language support should benefit both app developers and app users. Additionally, in CCPA [Regs §999(a)(2)(d)], it stipulates that online notices should follow generally recognized industry standards, such as the W3C Web Content Accessibility Guidelines . We use ( ) to denote that APPG can generate privacy policies in more than one language, otherwise (#).
Readability (10). While a long privacy policy can provide more comprehensive and detailed descriptions of data practices in apps, adding unnecessary information to a privacy policy may lead to information overload . Although APPGs can generate more concise privacy policies that are more specifically tailored to the data and data practices relevant to each app, this does not necessarily mean that those privacy policies are more readable. Some regulations require mobile app developers to use clear and understandable language in privacy statements . Also, W3C Web Content Accessibility Guidelines [§3] requires that “make text content readable and understandable”. However, those high-level principles do not give details for this requirement. Thus, we adopt the Flesch Reading-Ease Test  to evaluate the readability of generated privacy policy context. A higher score indicates the content is easier to read and understand, the score scales from 0 to 100. The average readability score of the privacy policies of 12 leading mobile apps , who have over one billion installs, is 46. We use ( ) when the readability score is greater than or equal to 46, (G) for 40 to 46, and (#) for less than 40. Specific readability scores are listed in Appendix Table 10.
Since the assessment of apps in these dimensions involved some subjective judgments, the first two authors assessed the 10 APPGs individually. For any disagreement at the table, they discussed and agreed on the same answer, and if the disagreement persisted, a third author joined the discussion to facilitate a resolution. As shown in Table 2, the level of Complexity is positively associated with the level of Extent of Data Use, Customizability, and User Instruction. Most of the APPGs (8/10) exhibit complete publishing support. As for compliance with privacy regulations, more than half the APPGs (6/10) claim that their privacy policies conform to the GDPR and CCPA, and only #1 Iubenda claims compliance with LGPD. Only #1 Iubenda supports generating privacy policies in languages other than English. All APPGs’ readability scores are between 30 and 50, which indicates that readers should have at least a college-level education background to easily read the policies . #2 App Privacy Policy Generator has the highest readability score and is the most frequently used APPG on the current market, as discussed in Section 3.
# 2 Extent of Data Use Recognised by APPGs
In this section, we further investigate the extent of data, data use, and data practices recognised by APPGs to support the generation of privacy policies. We identified four major aspects: the app’s basic information, users’ personal information, device permissions, and third-party services. We carefully gathered all the possible questions and potential options that APPG users might face. If any question or option explicitly pertains to a specific data type, permission, or third-party service, then it is classified as “recognised” ( ), otherwise “absent” (#). The specific item enumeration process is available at .
App’s basic information. Apart from user names, providing at least one communication channel to app users.
The first row of each section is the number of APPGs as per Table 1. “Multiple-choice questions” refers to whether developers select options from listed items, and “Completion questions” means developers fill in blanks with plain text. (Selecting a date is also regarded as a completion question.) Since some questions sometimes unlock later questions, we use “Minimum” and “Maximum” to reflect the lower and upper bounds. *If date of birth is included in the APPG, then we consider that Age group can be inferred.
for any potential inquiries is commonly required by privacy protection and regulation laws, and all the APPGs support this with different levels of granularity.
Users personal information. We manually separate this into two sub-categories: general personal information, and sensitive personal information. Most APPGs recognise general personal information, and only some APPGs (3/10) explicitly identify sensitive information.
Device permissions. According to the Android developer’s guide , there are nine dangerous permission groups, namely CALENDAR, CAMERA, CONTACTS, LOCATION, MICROPHONE, PHONE, SENSORS, SMS, and STORAGE, on mobile phones. We expect the collection and use of data related to these permissions to be clearly declared in a privacy policy, therefore, they are set as row indices. We found that more than half of the APPGs (6/10) recognise CAMERA, CONTACTS, and LOCATION, and only two APPGs identify all permissions. Failing to correctly state sensor permissions in the privacy policy could lead to serious privacy issues, so we further investigated whether users can correctly utilise the APPG to state all claimed sensor permissions in Section 4.
Third-party services. Third-party services have become a significant part of mobile apps. Taking into account the diversity of service providers and their popularity, we selected five third-party services as row indices, as most APPGs either fully or partially cover them. Overall, APPGs show decent coverage among these third-party services.
Statistic summary. We counted the completion questions and multi-choice questions (examples are illustrated in Figure 1) that users would encounter for the questionnaire-mode APPGs. Although questions may vary dramatically, we take it as a quasi-indicator to approximately reflect users’ learning and time cost to use the APPGs.
As it involves intensive manual work, to avoid the effect caused by potential human error, we employ the same strategy as introduced in the previous section. Cohen’s Kappa  κ = 0 for the initial manual labelling, which is an almost perfect level of agreement. As shown in Table 3, #1, #3, and #8 are the best in terms of the recognised data use, they almost cover every data index listed in the table. #2, #4, #7, and #9 decently cover users’ general personal information and third-party services, but only a few or null users’ sensitive personal information and device permissions. #5, #6, and #10 merely cover some app’s basic information and failed to support the rest of the categories. Above all, users need to carefully select appropriate APPGs, otherwise, the unsupported data use will be missing in the generated privacy policy.
# Finding 1: APPGs are a handy solution for developers to draft privacy policies for their apps. They have various features, characterizations, and levels of recognition of data use.
# 3 Market Penetration of APPGs (RQ2)
The previous section carefully examined various characteristics of APPGs. In this section we consider the questions: How large is the APPG market? And are the seemingly more functional APPGs actually more popular in the market?
# 3 Status Quo of Mobile Apps’ Privacy Policy
Privacy policies play an essential role in mobile app ecosystems. An app without a privacy policy is not necessarily malicious, but developers are required to provide privacy policies due to regulations in some jurisdictions. These policies are also increasingly demanded by mobile app users and marketplaces. According to the Apple App Store Developers page , a privacy policy is required when submitting new apps or app updates: “By adding the following links on your product page, you can help users easily access your app’s privacy policy and manage their data in your app. Privacy Policy (Required): The URL to your publicly accessible privacy policy.” Additionally, in Google Play Console Help Center , developers “...must include a link to your privacy policy” to prepare the app for review. Although platform principles regulate the existence and quality of privacy policies, we still discovered some critical problems in the mobile apps market. Other forms of privacy notices, such as privacy nutrition labels (PNLs) , focus more on providing succinct information to end users, potentially sacrificing information relevant or required by specific regulations or marketplace requirements.
# 3 Crawled Privacy Policy Dataset
To better understand the status and quality of existing privacy policies, we need a large-scale privacy policy dataset. However, existing datasets suffer from various problems.
The differences between datasets for previous research and our research goals. Existing datasets are either not focused on mobile apps, biased, small, or outdated. First, some prior research focuses on privacy policies across all platforms, including websites and mobile apps , while we only target mobile apps. Since they did not include metadata (e.g., platforms) of the collected policies, we cannot filter them out from other platforms. Second, some research only focuses on apps of certain groups, and their collected datasets are not representative enough for apps in the market. For example, one study  manually collected 5,684 “globally popular apps.” This selection criterion leads to two critical problems: a) The top overall popular apps appear in a limited set of categories, such as Social and Finance, which can drive out apps from less popular categories such as Parenting or Libraries & Demo. b) These globally popular apps do not distribute evenly across various app categories, which may cause potential bias and unrepresentative findings. Third, some existing
datasets are small and can contain potential bias. For example, APP-350  only analysed 350 privacy policies, and another study  conducted fine-grained analysis on only 304 privacy policies. Finally, due to the rapid iteration of apps, apps’ privacy policies, and APPGs, some prior datasets can quickly become outdated. In summary, to make our findings comprehensive, generalizable, and up-to-date, we needed to collect a current large-scale dataset that can assemble apps of different popularity and from various app categories.
Google Play Store. Android mobile apps are often more transparent and friendly to academic research compared to iOS mobile apps . There are many app markets for the Android platform, but Google Play is the largest and the most accessible app market, with over two million apps, according to AppBrain . Thus, we only focus on it in this study. Some Android apps are specially designed for Android smartwatches instead of mobile phones; therefore, we intentionally excluded apps belonging to categories named Watch apps and Watch faces.
Our dataset. We collected a new large-scale privacy policy dataset from existing apps on the Google Play Store. We also collected app metadata from AndroZoo , which is a large-scale and growing Android app collection extracted from multiple sources, including the Google Play app store. We removed duplicates and apps from other app markets and then randomly sampled 268,500 apps as our initial app dataset, which is around 10% of the whole app population. From this initial sample, we observed that some apps are either invalid or not usable. These were mainly dummy apps or student apps with a package size normally less than 10 KB, so we removed invalid apps like these. We further excluded unavailable apps that returned error messages on their Google Play page. Apps can be unavailable for many reasons including geographic differences  caused by government censoring, or being removed by Google because of disruptive adware, malware, restricted content, or other reasons. After these exclusions, we were left with 99,194 usable apps and their metadata. We then employed google play scraper  to obtain app information including privacy policy links, app categories, and required device permissions. Based on the available privacy policy links shown on the app’s homepages, we further utilize the BeautifulSoup  and Selenium  python packages to download those websites. Eventually, we obtained 46,472 privacy policies, and we refer to this privacy policy dataset as “Crawled Privacy Policy Collection.” We conducted the above data-gathering process in March 2022.
# 3 Missing Links and Unavailable Privacy Policies
Providing a privacy policy is essentially required by platform principles and privacy regulations, however, we observed that 15% (15,572/99,194) of apps do not provide a privacy policy link on their Google Play homepage. A previous study  reported that this statistic was 49% in 2019. Seemingly, as more regulation has been introduced, more apps have provided a privacy policy. However, we found that 37% (37,150/99,194) of privacy policy links lead to unavailable websites with error messages such as “403 Forbidden” or “404 Not Found.” This problem could be caused by potential reasons such as deliberately providing a dummy link, an outage, or the removal of the website server. Hence, in addition to simply requiring app developers to provide the privacy policy, the mobile app market also needs to frequently and regularly verify the validity of provided privacy policy links. Also, a permanent hosted privacy policy link provided by APPGs can help developers to mitigate availability issues to some extent.
# 3 Low-Quality Privacy Policies
Several previous studies  reported that it was not rare that some apps only provide a low-quality privacy policy, and we observe similar issues in our dataset. Firstly, some privacy policies do not contain meaningful privacy-related context, or are just dummy websites. For example, “Bway App” is a free app that provides football results, statistics, trends, and match result predictions, with over 10,000 installs; however, their privacy policy website  does not contain any content, and the file size of the crawled privacy policy is close to 0 KB. In addition, some are apparent “homemade” privacy policies that are too general to include essential data practices. For instance, “Easy Communication” is designed to help people with autism, cerebral palsy, dyslexia, intellectual disability, and other special needs to communicate easily. Its privacy policy  only has 156 words, and ambiguously mentions required device sensor permissions without any elaborations. Based on our observations and previous work, we empirically set the file size threshold as 2 KB , and the document length threshold as 200 words . If a crawled privacy policy does not meet both criteria, it will be regarded as a low-quality privacy policy. In total, we identify 22% (10,375/46,472) low-quality privacy policies.
# 3 Observations on Status Quo
In this section, we report an analysis of app privacy policies and identify three common problems in existing apps: missing/unavailable privacy policies, low-quality privacy policies, and language non-localization problems. Although similar problems were discussed in several previous studies.
# 3 Language non-Localization Problem
Language localization is necessary to promote apps in a global market. Although app UI and content can be translated, app developers may neglect the privacy policy. We employed the Python package langdetect  to detect the language of the crawled privacy policy documents and found 20%
(9,523/46,472) of apps do not provide an English privacy policy, although these apps were released in markets where the primary language is English. Interestingly, the top five non-English languages are Spanish (15%), Portuguese (9%), German (8%), Korean (8%), and French (7%). This trend might be attributable to the implementation of the European GDPR and Brazilian LGPD. We observed this problem even with top apps from some large companies. For the privacy policies of the top 1,000 most installed apps (all > 500k installs) in our dataset, there are still around 14% that do not provide an English privacy policy. APPGs may be a solution to language non-localization problems. Only APPG #1 Iubenda supports multilingual generation, but some APPGs (#4, #7, and #9) have included placeholders for future options for language selection.
# 3 Synthetic Apps and Generated Privacy Policy Collection
To identify whether privacy policies could be generated by APPGs, we first need to build the ground truth with self-generated privacy policies for each APPG. As shown in Table 1, seven APPGs require users to register an account to use the service, and six of them also require users to pay subscription fees to unlock all features. We registered as required and paid subscription fees. Synthetic apps are commonly used in similar empirical studies such as . We designed and tailored 3 synthetic apps specifically based on APPGs’ features and characteristics as summarised in Table 2 and Table 3. This was so that the boilerplates and pre-defined clauses of every APPG can be fully explored and reflected in the generated privacy policies. The functions and features of these synthetic apps are as follows:
- Synthetic App 1. A toy-like app that collects only general personal information and does not need to comply with GDPR, CCPA, or LGPD.
- Synthetic App 2. A social app that enables people to interact and communicate with others. Users need to create an account by providing their general personal information, and the app requires all device permissions. The app also accesses third-party services and only needs to comply with GDPR.
- Synthetic App 3. A hypothetical omnipotent app that requires users to provide all general and sensitive personal information to function. The app also needs all device permission and access to third-party services. This app needs to meet the requirements of GDPR, CCPA, and LGPD.
These three synthetic apps are various in terms of functionality and sophistication, and they decently cover all data use mentioned in Table 3. Therefore, they are capable of being the boilerplate apps for APPGs’ ground truth in the majority of cases. For each APPG, we created the same three custom privacy policies to test whether a privacy policy can be generated by one of analysed APPGs. Given ten APPGs and three synthetic apps, we obtained 30 privacy policies as “Generated Privacy Policy Collection” in total. We utilized it and “Crawled Privacy Policy Collection” to conduct the following market penetration analysis.
# 3 Market Penetration Analysis
We report a market penetration analysis to highlight the extent to which APPGs are used and to provide insights about why some APPGs are more popular. Specifically, we employ two methods to detect whether a privacy policy could be created by one of these APPGs: fingerprint keyword searching and document similarity comparison.
# Fingerprint Keyword Searching (FKS)
APPGs offer various publishing features, including the provision of a direct link as a permanent website host or an editable HTML document. We observe that to better advertise themselves, APPGs typically embed their company names into the URLs of the generated privacy policy websites or the editable HTML document. Consequently, we construct the fingerprint keyword set grounded in this feature. To cultivate this set, we manually inspected these APPGs and the collection of generated privacy policies. To mitigate the likelihood of false positives, we verified the authenticity of all keywords individually. Specifically, we incorporated a candidate fingerprint keyword into the set only if it appeared consistently across all three ground-truth privacy policies crafted for synthetic apps. All keywords are deliberately sensitive to case and format. The full keyword list is available at .
Then, the collected keyword set is employed to perform FKS in the crawled privacy policy collection. For each privacy policy in our collection, if we find a match in either its website link or its document context with one of the fingerprint keywords, the policy is considered to be generated by the corresponding APPG. For example, during the examination of policy website URLs, we use the keyword “iubenda.com” to search URLs like "www.iubenda.com/123abc..." for APPG #1 Iubenda. Similarly, while scrutinizing policy HTML documents, identifiable phrases like "iubenda hosts this content" were utilized as markers.
# Document Similarity Comparison (DSC)
As we mention in Section 2, some APPGs allow users to customize the content to some extent, and APPG users may also further adapt or polish the initial generated privacy policy in editable HTML format before publishing. Consequently, the fingerprint keywords, even sentence-level segments, are not necessarily included in a generated app privacy policy sometimes.
# 4 Assessment of Privacy Policies (RQ3)
Generators are designed to generate compliant privacy policies. This section reports a detailed assessment of generated privacy policies.
# 4 Privacy Compliance Against Regulations
APPGs need to be designed in a way that allows developers to produce compliant privacy policies. Furthermore, they must be updated to reflect evolving laws. In Table 2, six APPGs claim that they are able to generate policies in compliance with CCPA and GDPR, and only #1 Iubenda mentions compliance with LGPD. However, their actual compliance with generated policies may be different from what they claim. Zimmeck et al.  conducted two studies in May 2020 and January 2021, revealing that some APPGs had significant compliance issues, such as failing to create CCPA-compliant policies. We perform a follow-up inspection in May 2022 based on generated privacy policies by Synthetic App 3, since
# Results
We employed the proposed methods to provide an estimation of the market penetration of APPGs. Among 46,472 valid privacy policy documents from apps at the Google Play Store, as shown in Table 4, we found 6% (3,066) privacy policies are highly likely generated by one of the APPGs by using fingerprint keyword searching and 18% (8,425) by using document similarity comparison. We also checked the intersection size, which is 4% (2,042), indicating that these two methods are complementary to each other. The union of the previous two methods is 20% (9,332), indicating an upper bound on possible market occupancy ratio. Overall, the results show that APPGs do play a considerable role in the current mobile app market.
# Finding 2
The market occupancy ratio of 10 examined APPGs is around 20%, and #2 App Privacy Policy Generator is the most popular one, boasting a 72% adoption rate. Moreover, users tend to select easy-to-use APPGs even though at the cost of a potentially-higher risk to breach privacy regulations.
The number of privacy policies generated by APPGs, detected by FKS (left), DSC (mid), and both (right).
The individual requirements of LGPD are shown in Table 5. “N.R.” stands for “no record”. The enforcement date of LGPD is September 2020.
They are supposed to comply with all compliance requirements of GDPR, CCPA, and LGPD Table 6 shows tallies of generators’ compliance with legal requirements in May 2020, January 2021, and May 2022 for GDPR, CCPA, and LGPD.
There is a notable discrepancy between the APPGs’ claimed ability to generate policies compliant with specific regulations and their actual compliance levels. In addition, across the table, compliance with GDPR and CCPA requirements improved over time from May 2020 to May 2022 for several APPGs (#3, #4, #7, #9), demonstrating an adaptation to regulatory standards. An encouraging observation is that APPGs (#1, #3, #4, #7, #8, #9) maintained full GDPR and a majority of CCPA compliance since January 2021. This might be due to the fact that GDPR has been in effect since 2018, providing APPGs with more time to adapt their policies accordingly. Given that LGPD came into effect in September 2020, it is not unexpected that compliance levels would be relatively low in the initial stages. The findings underscore the necessity for APPGs to improve their ability to produce compliant privacy policies consistently across all major regulations. The results also call for ongoing monitoring and evaluation of APPGs to ensure they keep pace with evolving laws and maintain transparency with their users regarding their actual capabilities.
APPGs have varying levels of compliance when it comes to data collection practices. For instance, while GDPR emphasises "Categories of personal data concerned", CCPA requires to provide a detailed list of categories of personal information that should be disclosed if collected. APPGs #1, #3, #4, and #7 demonstrate compliance with GDPR’s requirements, but when it comes to CCPA’s detailed disclosure requirements, only APPGs #3, #7, and #9 comply. As for third-party data-sharing and data-sale practices, CCPA’s emphasis is on "List of categories of personal information collected and sold" highlights this; whilst GDPR identifies “Recipients or categories of recipients of the personal data”. APPGs #1, #3, #4, and #7 have clear disclosures in line with third-party disclosure, but others like #2 and #5 lag behind. Overall, APPGs #3 and #7 consistently fare well across all three regulations, indicating a comprehensive privacy policy generation. However, APPGs #2, as the most popular, display apparent gaps compared to others.
# Data Right Coverage Analysis.
Regulations often necessitate data rights to grant sufficient choice and control to the end users over the personal data collected by businesses. Thus, it is important to assess whether the generated policies clearly disclose data rights. For this, following [GDPR Art] and
[CCPA §1798(a)(5)(A)]2, we extracted and summarised seven fundamental data rights. We manual examined the existence of disclosure for each data right of 10 APPGs. For comparison, we also scrutinized 12 leading mobile apps, which are mentioned in Section 2-Readability. These 12 leading apps are in various categories and published by different companies, with over one billion installations. Table 7 shows that almost all leading apps disclose fundamental data rights in their privacy policies. In addition, results validate the respective assertions made by APPGs, regarding their self-claimed compliance with GDPR and CCPA. Notably, #2 as the most popular APPG, does not disclose any of the data rights in the generated policy document.
Privacy Practice Disclosure Analysis. In alignment with the principles of data protection and transparency, it is also imperative that privacy policies encompass disclosure of privacy practices. Based on  and , this study delineates four highly concerning privacy practices. These are: 1) Data Encryption: Users’ data are encrypted and transferred over a secure connection. 2) Government Requests: It refers to the potential for government entities to request access to users’ data for various reasons, including national security or criminal investigations. 3) Data Breach Notification: In the event of a data breach where users’ information is compromised, it is essential to have a mechanism in place to notify affected users promptly. 4) Changes Notification: The privacy policy should mention the procedures for notifying users about significant changes to the policy, including how and when users will be informed. Then, we manually scrutinized the existence of disclosure across 10 APPGs and the same 12 leading apps. Table 8 shows that more than half APPGs disclose Data Encryption and Changes Notification. Five APPGs include Government Requests and only #8 includes Data Breach Notification in the generated policy document (“In the event of a data breach, we will make reasonable efforts to notify affected individuals...”). In addition, #5 and #6 do not include any privacy practice, and #8 includes all items. Furthermore, #2 as the most popular APPG, only include Data Encryption.
# Finding 3:
Noncompliance with privacy laws and under-claiming issues indicate potential need for a more stringent evaluation of APPGs’ capabilities.
# 4 Permissions Coverage Analysis
Device (sensor) permissions enable features of mobile apps and are normally critical in terms of personal privacy risks. Intuitively and legally, an app’s privacy policy should accurately disclose its permission usages; however, it is common that developers do not list all the permissions of their app in the privacy policy . Developers of mobile apps in the Google Play app store are required to clearly list the permissions they intend to obtain on their homepages3, and we denote them as Pclaimed. By taking the self-reported device permission usage as the ground-truth, we then assess whether the APPG’s provided UIs, questionnaires, or documents can sufficiently allow the APPGs users to enter permission invocation information by their app. Notably, the self-reported device permission usages by developers may not actually reflect the actual privacy behaviour of apps.
To scrutinise how developers translate used device permissions into policies by APPGs, for each permission (per), we use it and Synthetic App 1 to generate a privacy policy (PPApp1+per). By comparing the differences between PPApp1+per and PPApp1, we can locate and obtain the phrases/sentences each APPG uses to declare each permission in the privacy policy. For each app, by searching and counting the corresponding phrases/sentences in its privacy policy, we obtain the number of permissions displayed and denoted as Pdisplayed. For each app that uses the APPG, its Precognised is equal to the recognised dangerous permissions enumerated in Table 3. Then, we introduce two metrics for the extent to which APPGs cover claimed permissions in the generated privacy policy, as follows:
Recognised Coverage (RC) = 1∑PrecognisednPdisplayed
A higher RC indicates the app developers use the APPG more.
3This feature was deprecated as Google launched the new Data Safety after we completed this work.
Expected Coverage (EC) = n
A higher EC reflects the APPG has less intrinsic design flaws about disclosing permissions in generated privacy policies.
We use the intersection of privacy policies identified by both FKS and DSC (2,042 privacy policies) because it is more certain that they are generated by one of the APPGs. Based on our observations in Section 2, four APPGs (#2, #5, #6, #10) do not support permission declarations, in other words, Precognised = 0 for every app identified to use those APPGs. In addition, we find that Pdisplayed = 0 for four of those apps, which means that none of the developers realised that the APPG they used did not cover the permissions they claimed.
The results are shown in Table 9. First, for #1 and #3, they both can recognise all nine device permissions involved in this study, so their RCs are equal to ECs. There is a significant portion of missing permission disclosure for all APPGs, since all ECs are much smaller than 100%. The missing disclosure issue is not dependent on users’ input, but on the design of APPGs that did not sufficiently accommodate the requirements. Second, compared to #1’s and #3’s consistency on RCs and ECs, for #4, #7, #8, and #9, their ECs are greatly smaller than RCs. The gap shows the importance of APPG selection, because inappropriate APPGs will hinder developers from including self-reported (claimed) permissions in their privacy policies. Third, #1 has the lowest RC, but it has high level user instruction; whereas #4, #7, and #9 has the relatively high RCs, but low level of user instruction. This counter-intuitive result indicates that current user instructions are not helpful enough to guide developers to correctly include permissions in the generated policies. In addition, #1 is the only UI-mode APPG, with the lowest RC (15%), and other comparable APPGs, whose RC is over 50% are all questionnaire-mode. This observation indicates that UI mode APPGs may be more challenging to be properly used by users.
# Finding 3:
A significant portion of device permissions remain under-claimed, perhaps caused by APPGs’ design issues, and questionnaire mode can better guide users to include claimed device permissions compared to UI mode.
# 4 Contradiction Analysis
Previous works have discussed contradictions within a policy . Specifically, if affirmative and negative sentences mention the same or conflicting entities and data types in a policy, then there can be a contradiction. In this section, we compare the APPG-based and non-APPG-based privacy policies, to examine the situation of contradiction issues in both and the effect introduced by APPGs. For non-APPG-based privacy policies, we used the test set from PoliCheck  which contains 200 policies randomly sampled from 13K mobile apps. For APPG-based privacy policies, we use 30 policies from the “Generated Privacy Policy Collection” and 170 policies that are randomly sampled from the intersection list (indicated as APPG-generated by both FKS and DSC). We employ the state-of-the-art privacy policy analyser, PoliGraph , to conduct the following analysis.
We observed 26 contradictions in APPG-based privacy policies and 15 in non-APPG-based policies, indicating a potential tendency for APPGs to introduce more privacy policy contradiction issues. Closer examination revealed that a significant portion of these contradictions in APPG-based policies arise from conflicting statements present in different sections of the document. For example, one section of the policy explicitly states “[A condition], we sell your personal information to third parties”, while an assertion in the CCPA section contradicts this by claiming “[The App] has not disclosed or sold any personal information to third parties for a business or commercial purpose in the preceding 12 months.”. Therefore, it is imperative for APPG users to exercise caution and meticulously evaluate the coherence and consistency between different sections of generated privacy policies.
# Finding 3:
More privacy policy contradiction issues exist in APPG-based privacy policies.
# 5 Discussion and Implication
Challenges and opportunities coexist in the current APPG development. Based on our observations and study results, we summarize some findings for various roles or stakeholders in the APPG ecosystem.
App developers/APPG users. While app developers may benefit from using APPGs to create privacy policies more efficiently, they should be aware of APPGs’ latent limitations. As illustrated in Section 2, APPGs have different qualities. Some do not directly target specific regulations, such as GDPR and CCPA, and some do not recognise data practices, i.e., the declaration of personal information, device permissions, and third-party services used in their mobile apps. Since app developers must make sure the privacy policies they provide are comprehensive, readable, and compliant, it is very likely a trap if they do not select and use appropriate APPGs.
# 6 Related Work
Privacy policy analysis. Privacy policies have been extensively analysed and discussed by the research community . Wilson et al.  created a corpus of 115 privacy policies and 23K fine-grained data practise annotations, and revealed users’ preferences on privacy policy structure and complexity. Amos et al.  reported that privacy compliance and readability were worse in the last 20 years, according to 130k website privacy policies. Andow et al.  presented PolicyLint, which is a privacy policy analysis tool that can identify privacy contradictions by simultaneously considering negation and varying semantic levels of data objects and entities. Bui et al.  proposed an automated system, dubbed PurPliance, that detects inconsistencies between the data-usage purposes stated in a privacy policy and the actual behaviours of an Android app.
Code-centric privacy policy auto-generation tools. Yu et al.  developed a system named AutoPPG, to automatically construct readable descriptions from the source code of mobile apps, to help create privacy policies on Android. Rocky Slavin  developed PoliDriod, an Android Studio plugin that can be used to detect possible misalignments between Android API methods and privacy policies. Zimmeck et al.  proposed a privacy policy generator, named PrivacyFlash, which leverages mappings between code signatures and privacy practises expressed in policies for iOS apps. While code-centric generators might better align with apps’ actual privacy behaviours compared to online APPGs, they come with significant challenges that hinder their adoption by app developers. First, they cannot ensure compliance with high-level privacy regulations, particularly non-functional requirements. Second, they present a higher entry barrier for developers due to their inherent complexity, whereas online APPGs can be easily accessed, offering a range of user instructions and publishing support as demonstrated in this study. Others  also discussed the automated generation of privacy policies using machine learning methods.
# 7 Conclusion
Online Automated Privacy Policy Generators (APPGs) are broadly used by developers of mobile apps to create privacy policies to respond to regulatory requirements. This paper reports the first large-scale empirical study to comprehensively scrutinize APPGs’ various features, characteristics, and extent of recognition of data use. Our market penetration analysis indicates that privacy policies of 20% apps could be generated by existing APPGs on the Google Play app store, and that #2 is the most popular APPG, with a 72% adoption rate. Our findings underline a substantial level of noncompliance with privacy laws and a frequent under-claiming of data rights and highly concerning privacy practices, especially with the most popular APPG #2. Permissions coverage analysis reveals that existing APPGs have significant issues with including all essential device permissions in the generated privacy policy, and UI mode APPGs could make it worse. Also, more contradiction issues exist in APPG-based privacy policies. In summary, for app developers, selecting and employing APPGs without careful consideration is very likely a trap, creating a substantial risk of breaching privacy regulations.
# Appendix
# Readability Analysis
The readability score of privacy policies are calculated by the Flesch Reading-ease Test :
206 − 1 total words − 84 total syllables
total sentences total words
Specific readability scores are listed in the Table 10.
# GDPR and CCPA requirements compliance
The specific requirements and compliance checking for GDPR and CCPA are tallied in Appendix Table 11 and Table 12.