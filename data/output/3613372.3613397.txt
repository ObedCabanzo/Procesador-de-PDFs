# Applying Spectrum-Based Fault Localization to Android Applications
Euler Horta Marinho
Federal University of Minas Gerais
Brazil
eulerhm@dcc.ufmg.br
Fischer Ferreira
Federal University of Ceara
Brazil
fischer.ferreira@sobral.ufc.br
João P. Diniz
Federal University of Minas Gerais
Brazil
jpaulo@dcc.ufmg.br
Eduardo Figueiredo
Federal University of Minas Gerais
Brazil
figueiredo@dcc.ufmg.br
# ABSTRACT
The pressing demand for high-quality mobile applications has a major influence on Software Engineering practices, such as testing and debugging. The variety of mobile platforms is permeated with different resources related to communication capabilities, sensors, and user-controlled options. As a result, applications may exhibit unexpected behaviors and resource interactions can introduce failures that manifest themselves in specific resource combinations. These failures can affect the quality of mobile applications and degrade the user experience. To reduce human effort of manual debugging, several techniques have been proposed and developed aiming to partially or fully automate fault localization. Fault localization techniques, such as Spectrum-based Fault Localization (SBFL), identify suspicious faulty program elements related to a software failure. However, we still lack empirical knowledge about the applicability of fault localization techniques in the context of mobile applications, specifically considering resource interaction failures. To address this problem, this paper evaluates the use of SBFL aiming to locate faults in 8 Android applications and verify the sensitivity of SBFL to variations in resource interactions. We rely on mutation testing to simulate faults and on the Ochiai coefficient as an indicator of the suspicious faulty code. Our results indicate that SBFL is able to rank more than 75% of the faulty code in 6 out of 8 applications. We also observed that the ranking of suspicious code varies depending on the combination of enabled resources (e.g., Wi-Fi and Location) in the mobile applications.
# CCS CONCEPTS
• Software and its engineering → Software testing and debugging.
# KEYWORDS
mobile applications, resource interactions, fault Localization, SBFL
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
SBES 2023, September 25–29, 2023, Campo Grande, Brazil
© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0787-2/23/09. . . $15
https://doi.org/10/3613372
# ACM Reference Format:
Euler Horta Marinho, Fischer Ferreira, João P. Diniz, and Eduardo Figueiredo. 2023. Applying Spectrum-Based Fault Localization to Android Applications. In XXXVII Brazilian Symposium on Software Engineering (SBES 2023), September 25–29, 2023, Campo Grande, Brazil. ACM, New York, NY, USA, 10 pages. https://doi.org/10/3613372
# 1 INTRODUCTION
The growth of the mobile application market, for instance, due to the popularity of application store, has brought new challenges to their development and testing. For instance, the pressing demand for high quality applications has an important influence on Software Engineering practices, such as testing and debugging . Testing is one of the most important approaches to quality assurance in the field of mobile applications, as evidenced by several secondary studies . For a proper software testing, we also need debugging which is another quality assurance activity aimed at the localization and removal of faults . Nevertheless, manual debugging can be extremely challenging, tedious, and costly, since it relies heavily on the software developer experience, judgment, and intuition to identify and prioritize code that is likely to be faulty . Therefore, developing techniques have been proposed aiming to partially or fully automate fault localization while reducing human effort .
Fault localization techniques aim to identify faulty program elements related to software failures using static or run-time information to determine the root cause of the failure . Some of these techniques, such as Spectrum-based Fault Localization (SBFL), can produce a ranked list of suspicious code elements for developers, reducing their effort for manual fault checking . Intuitively, the more a code element is executed by failing test cases, the more suspicious it is . An SBFL technique often calculates suspiciousness scores using a ranking metric also known as a risk evaluation formula . Ochiai , DStar , and Tarantula  are among the most common metrics for this purpose .
Several techniques have been proposed and evaluated for fault localization . Nonetheless, these techniques must be assessed for mobile applications, since they may demand tailored quality assurance approaches due to mobile specific characteristics . For instance, debugging mobile applications is challenging and the localization of the faulty code may not even be apparent from the stack trace .
# SBES 2023, September 25–29, 2023, Campo Grande, Brazil
Marinho, et al.
Mobile applications typically run on a variety of platform configurations . Each platform configuration relies on a different set of enabled platform resources making application testing and debugging more challenging. Application resources can be related to communication features (e.g., Wi-Fi and GPS), sensors (e.g., Accelerometer and Gyroscope), and user-controlled options (e.g., Battery Saving and Do Not Disturb). Some of these resources can be managed directly through system-level settings, such as the Android Quick Settings1, which allow the user to customize many system or application behaviors . However, applications may exhibit unexpected behavior due to failures that manifest themselves in certain combinations of enabled resources . According to Sun et al. , failures involving two resources are critical but not very common in mobile applications. Another study  found a greater number of this type of failure.
In this work, we evaluate the use of the SBFL technique  aiming to locate faults in Android applications and verify the sensitivity to resource interaction failures. We use faults seeded from mutation operators in order to conduct the experimental study and rely on the Ochiai coefficient as an indicator of the suspicious faulty code . Although there are many metrics for calculating the suspiciousness score, the Ochiai coefficient is considered one of the metrics with the best performance . Despite the Ochiai coefficient being preliminarily used in mobile applications , we still lack knowledge about its applicability in the context of open source mobile applications, especially with respect to resource interaction failures, since the SBFL was not designed for these types of failures. Testers and developers may neglect to properly test and debug mobile applications considering resource interactions because they lack knowledge about such failures . In consequence, these failures may occur during everyday use of the mobile application, while they are not noticeable during the testing and debugging activities.
To achieve our goal in this study, we follow four steps. First, we select 8 open source applications from GitHub used in our previous study . Second, we use a tool  to generate mutants for each target application. We generate mutants for two groups of classes, i.e., classes that use APIs2 of resources (resource-related classes) and classes that do not use such APIs (general classes). Third, we execute the test suites for each mutant and collect code coverage metrics. We then investigate the sensitivity of SBFL when there are variations in resource settings, a known source of application failures as demonstrated in previous studies . Finally, we analyze the test reports to calculate the Ochiai coefficient for each application aiming to locate the faulty code. Our analysis is performed at the method-level. Therefore, SBFL reports the suspiciousness score for the methods.
Our results indicate that SBFL is able to rank more than 75% of the faulty methods for 6 applications. However, there is no evidence of a difference in the ranking coefficient between faults in resource-related classes and faults in general classes. Regarding the sensitivity of SBFL to variations in resource settings, we found a major influence of resource settings on the suspiciousness score.
# 2 BACKGROUND
In this section, we present an overview of concepts of Spectrum-based fault localization (Section 2), mutation testing (Section 2), and resource interactions in mobile applications (Section 2).
# 2 Spectrum-based fault localization
SBFL is a technique based on the analysis of the program spectra or coverage , i.e., the program elements covered during a test execution . These elements can be of different granularity level, e.g., statements, blocks, predicates, methods, etc. We focus on method-level fault localization in this paper. Many SBFL techniques use ranking metrics to associate a suspiciousness score to the program elements. These techniques produce as output a list of elements ranked in descending order of suspiciousness .
The metrics used to calculate the suspiciousness score are the major concern for the design of a SBFL technique. Several metrics have been proposed to indicate faulty elements . Tarantula  was the first metric proposed exclusively for fault localization. The Ochiai  coefficient was adapted from Molecular Biology. Often, the metrics are defined in terms of four values collected from the execution of the tests :
- 𝑒𝑓: number of failed tests that execute the program element.
- 𝑒𝑝: number of passed tests that execute the program element.
- 𝑛𝑓: number of failed tests that do not execute the program element.
- 𝑛𝑝: number of passed tests that do not execute the program element.
For example, Ochiai uses the following formula for calculating the suspiciousness of a program element:
𝑂𝑐ℎ𝑖𝑎𝑖(𝑒𝑙𝑒𝑚𝑒𝑛𝑡) = 𝑒𝑓 / √((𝑒𝑓 + 𝑛𝑓) * (𝑒𝑓 + 𝑒𝑝)) (1)
1 support.google.com/android/answer/9083864?hl=en
2 LocationManager is an example of API for location resources
3 https://github.com/labexp/osmtracker-android
4 https://developer.android.com/reference/android/location/LocationListener
# Applying Spectrum-Based Fault Localization to Android Applications
# 2 Mutation testing
Mutation analysis is the process of introducing syntactic variations in a program aiming to produce program variants (mutants), i.e., generating artificial faults . Mutation testing refers to the use of mutation analysis in order to assess the quality of a test suite. When a test case shows the behavior of a mutant to be different from that of the original program, the mutant is said to have been “killed” or “detected” . Otherwise, the mutant is said to be “live”. During this analysis, we measure the number of mutants that are killed and calculates the ratio of those over the total number of mutants. This ratio is called mutation score .
The syntactic variations of mutation analysis is performed by means of “mutation operators” . A basic set of mutant operators, usually considered as a minimum standard for mutation testing , is the five-operator set proposed for the Mothra mutation system . This set includes the Relational Operator Replacement (ROR), Logical Connector Replacement (LCR), Arithmetic Operator Replacement (AOR), Absolute Value Insertion (ABS), and Unary Operator Insertion (UOI) operators. For example, let’s consider the snippet of the onLocationChanged method in Figure 2. This method receives a Location object as an argument and updates the location in line 12. A time stamp defined by lastGPSTimestamp is maintained to control the location update (line 11). We use the ROR operator for mutating the relational operator of line 3 and generate 5 mutants (lines 5-9): 𝑒𝑥𝑝𝑟 1 <= 𝑒𝑥𝑝𝑟 2, 𝑒𝑥𝑝𝑟 1 > 𝑒𝑥𝑝𝑟 2, 𝑒𝑥𝑝𝑟 1 >= 𝑒𝑥𝑝𝑟 2, 𝑒𝑥𝑝𝑟 1 == 𝑒𝑥𝑝𝑟 2, and 𝑒𝑥𝑝𝑟 1 != 𝑒𝑥𝑝𝑟 2.
Recent studies  have presented specific approaches for the mutation testing of mobile applications. For example, specialized mutation operators have been proposed from the taxonomy of real faults of Android applications . Moreover, cost reduction techniques for the mutation testing of Android applications were catalogued as a set of good practices .
# 2 Resource interactions in mobile applications
Resource interaction failures have only been recently explored in mobile applications testing . These failures occur when resources affect the behavior of other resources, similarly to the feature interaction problem in configurable software systems  and telecommunication systems . An example of resource interaction failure occurs for Wikimedia Commons app . Figure 3 presents a code snippet involved in a failure described in an issue.
The Android platform uses the GPS or the network (Wi-Fi/Mobile data) to determine the device location. This application fails if both GPS and the network are disabled. The failure is caused by calling getLastKnownLocation to get the current location over the network (line 3). However, this call returns a null value, which is later used to store the location-based values when constructing an object (line 5). As a result, the application crashes due to a NullPointerException. The issue was closed with a proper correction.
The high number of input combinations is a challenging aspect for testing software systems in general, since the effort of the exhaustive testing is generally prohibitive. Particularly, it is also the case of configurable systems  in which all tests must be executed with several configurations. Our previous work  named a input combination as a setting, i.e. a set of resources whose states (enabled or disabled) are previously defined. For instance, our previous study  considered a set of 14 common resources: Auto Rotate, Battery Saver, Bluetooth, Camera, Do Not Disturb, Location, Mobile Data, Wi-Fi, Accelerometer, Gyroscope, Light, Magnetometer, Orientation, and Proximity.
In this study, we evaluate the sensitivity of SBFL to variations in resource settings considering the same set of resources. That is, we investigate if SBFL is able to detect variations of combinations of resources, allowing us to locate the faulty code behind the resource failures.
# 3 STUDY DESIGN
This section presents the experimental design of our study. Section 3 shows the research questions and Section 3 delineates each phase of the study.
# 3 Research Questions
The goal of this study is to evaluate the use of SBFL to locate faults in Android applications and verify the sensitivity to resource interactions. To achieve this goal, we address the following research questions:
- RQ1: To what extent SBFL can be used for mobile applications?
- RQ2: Is there a difference in the ranking coefficient for faults in resource-related classes and faults in general classes?
- RQ3: How sensitive is SBFL to variations in resource settings?
The first research question can be answered by applying the SBFL and measuring the suspiciousness score using the Ochiai coefficient. We use faults seeded by mutation operators to control the fault localization. To answer the second research question, the faults are seeded in two groups of classes, resource-related classes and general classes. The resource-related classes are identified based on the analysis of the imported packages . For the third research question, we compare the suspiciousness scores in the context of the variation of resource settings.
# 3 Study Steps
# SBES 2023, September 25–29, 2023, Campo Grande, Brazil
# Marinho, et al.
# Application: OSMTracker
# Test case outcomes (pass=v , fail=X)
X v X X v v X
1    void onLocationChanged(Location location) {
2     . . .
3      if ( (lastGPSTimestamp + gpsLoggingInterval) < System.currentTimeMillis() ) {
4
5       // mut1 :        if ( (lastGPSTimestamp + gpsLoggingInterval) <= System.currentTimeMillis() )
6       // mut2 :        if ( (lastGPSTimestamp + gpsLoggingInterval) > System.currentTimeMillis() )
7       // mut3 :        if ( (lastGPSTimestamp + gpsLoggingInterval) >= System.currentTimeMillis() )
8       // mut4 :        if ( (lastGPSTimestamp + gpsLoggingInterval) == System.currentTimeMillis() )
9       // mut5 :        if ( (lastGPSTimestamp + gpsLoggingInterval) != System.currentTimeMillis() )
10
11           lastGPSTimestamp = System.currentTimeMillis();
12            lastLocation = location;
13            if (isTracking) { . . . }
14       }
15      }
1    Location lastKL = locationManager.getLastKnownLocation(locationManager.GPS_PROVIDER);
2     if (lastKL == null) {
3             lastKL = locationManager.getLastKnownLocation(locationManager.NETWORK_PROVIDER);
4     }
5    return LatLng.from(lastKL);             // An object is constructed from the latitude and longitude coordinates
the settings, since other studies 29 showed that this number of test executions was sufficient to detect flaky tests. The first application set was used to generate mutants (Step 3). For each mutant of the first application set, we execute the test suites (Step 4). This phase also includes the execution of the test suites of the second application set. Finally, we analyse the recorded test reports to calculate the Ochiai coefficient (Step 5). In the following sections, we detail each step.
# 3 Application Selection
Based on our previous work 29, we randomly selected 8 applications that meet the following criterion: implemented in Java and with test code size greater than 500 LOC. Table 1 depicts an overview of the selected applications. These applications are from different categories with a large variation of size (from 14,499 LOC to 347,897 LOC), test code size (from 525 LOC to 3,674 LOC), and test cases (from 4 to 164). We can observe a relative low instruction coverage of the test suites (from 2% to 51%). The column “Resources” presents resources declared in the Manifest file 8 that are used for the application selection. Some resources are not declared since they are not directly used by the application (for instance, Auto Rotate and Battery Saver). Other resources do not demand a uses-permission tag and may not be explicitly required by the developer with a uses-feature tag (for instance, Accelerometer
8https://developer.android.com/guide/topics/manifest/manifest-intro
260
# Applying Spectrum-Based Fault Localization to Android Applications
# SBES 2023, September 25–29, 2023, Campo Grande, Brazil
# Selected Applications
and Gyroscope). In this case, other approaches for code analysis could be used for identifying additional resources. is the low coverage of the test suite (2% in Table 1), although this application has a large number of resource-related classes.
# 3 Mutants Generation
We were not able to find specific mutation tools or operators for resource interaction failures. Therefore, we decided to use a generator prototype tool  that implements four of the five operators of the Mothra mutation system . This tool is able to generate mutants for Java code using the set of three mutants operators shown in Table 2. We opt for this tool since it requires less effort for setup execution and log generation and makes it simple to control the generation and execution of mutants.
These mutation operators (AOR, ROR, and LCR) are a subset of the five representative ones . They act on binary expressions and replace the language operator (arithmetic, relational, or logical) with other syntactically similar operators. Although the mutation operator SBR (Statement Block Removal) was implemented in the used tool , we do not use this operator because SBFL techniques have limitations in locating faults related to missing code . These mutation operators are capable of reproducing faults related to resource interactions in mobile applications because we generate mutants for resource related classes. This strategy allows the modification of the code related to the resource, as seen in the example of Figure 2 in a similar way to the mutation generation strategies in other studies .
We generate all mutants to each target application based on the selected operators, since the tool uses the concept of metamutant  to encode all mutants and the original source code into one application. In this way, the compilation and loading time is reduced, because all mutants can be enabled/disabled at runtime. Thereafter, we conducted a previous analysis to identify mutants covered by at least one test case.
The number of mutants generated is constrained by the SBFL approach, where each test case is executed individually, increasing the testing effort. We attempt to select 20 mutants (10 mutants for resource-related classes and 10 mutants for general classes) in each target application. The number of mutants for each application can be found in Table 3. However, we could not generate a uniform number of mutants for some applications due to a lack of mutation points. The mutants were randomly selected from the mutation operators set (AOR, ROR, and LCR). We restricted our study to 20 mutants due to experimental time constraints.
As we can notice, we are not able to seed all mutants for resource-related classes for Ground and Threema. For Ground, we only identify 5 mutants for this kind of class. A possible reason for Threema.
# 3 Test Suite Extension
Similar to our previous work , we instrumented code aiming to control 14 common resources of the Android Platform: Auto Rotate, Battery Saver, Bluetooth, Camera, Do Not Disturb, Location, Mobile Data, Wi-Fi, Accelerometer, Gyroscope, Light, Magnetometer, Orientation, and Proximity. The instrumentation is based on Android instrumented tests, i.e., a type of functional test . They execute on devices or emulators and can interact with Android framework APIs.
Each class of the test suites is extended with the instrumentation, allowing the control of specific contextual information of the resource states. The control of resources is based on settings. A setting is defined as a 14-tuple of pairs (resource, state) where state can be True or False depending on whether the resource is enabled or disabled.
# 3 Test Suite Execution
For each mutant of the first set of applications and the applications of the second set, we execute the test suites with the coverage reports enabled. We used a Xiaomi Pocophone F1 with 6 GB RAM, running Android 10. Since the calculation of the coefficient is based on the output of each test case, we execute test cases separately.
For illustrating the experimental effort, we collect a sample of the approximate execution time for all 20 mutants of each application in “Execution Time” column of Table 1. The CPU time was randomly sampled, since from our observation, we could not verify a great variation of time in the execution of the mutants. We can see that the execution time varies between 1h45m (OpenScale) to 1d3h (WordPress).
# 3 Coefficient Calculation
We analyzed test reports to identify failed test cases and the coverage reports to get the coverage information. In order to decrease the complexity of the analysis, we use the method coverage data for calculating the Ochiai coefficient of each method in target applications.
# 4 RESULTS
This section presents the study results and discusses them focusing on providing answers to the research questions. Section 4 provides.
9 https://developer.android.com/training/testing/instrumented-tests
# SBES 2023, September 25–29, 2023, Campo Grande, Brazil
# Marinho, et al.
# 4 Use of SBFL for Mobile Applications (RQ1)
The amount of dead mutants and the mutation score are related to the quality of the test suites, generally assessed by some code coverage criteria . We can note that AnkiDroid, Threema, and WordPress have mutation scores greater than 0 while the number of test cases oscillated between 54 and 164 (Table 1). However, their code coverage is between 2% and 19%. According to the ranked dead mutants, the SBFL is able to rank more than 75% of the dead mutants for 6 applications. A faulty method could not be ranked if there is no failed test case for it.
# 4 Coefficients in Resource-Related Classes and in General Classes (RQ2)
We analyse the Ochiai coefficients for resource-related classes (Group 1) and general classes (Group 2). Figure 5 shows the box plots of the coefficients. The horizontal axis presents the groups,
# Applying Spectrum-Based Fault Localization to Android Applications
# SBES 2023, September 25–29, 2023, Campo Grande, Brazil
# 4 Variations in Resource Settings (RQ3)
We select three instrumented applications to compare the suspiciousness score in the context of the variation of resource settings. The applications were selected considering the fact that they had failures manifested in three executions, an experimental procedure for dealing with flaky tests . We randomly select settings for each application from 214 possibilities that are able to cause failures in three executions to avoid flaky tests. For instance, SA for application Owntracks is 〈Wi-Fi, !MobileData, !Location, Bluetooth, !Camera, !AutoRotate, !BatterySaver, !DoNotDisturb, !Accelerometer, !Gyroscope, Light, Magnetometer, Orientation, Proximity〉, in which the exclamation mark indicates the disabled resource. We analyze the pairs of ranks and determine the number of methods ranked differently and calculate the percentage of difference in relation to the total number of ranked methods.
In this table, the settings id were labelled to make the presentation more clear.
# 5 DISCUSSION
We considered 8 applications with test suites created by the developers. In this way, the ranking of faults depends on the quality of these test suites. Since we investigate fault localization in the
# SBES 2023, September 25–29, 2023, Campo Grande, Brazil
# Marinho, et al.
context of resource-related failures by looking at RQ2 and RQ3 (Section 3), our analysis relies on instrumented tests, a type of functional tests (Section 3). Therefore, further efforts need to be made to extend the detection of these failures to other levels, e.g., unit and integration tests.
As we answered RQ2, there is no evidence of a difference between the two groups of classes. Therefore, we believe that the ranking of SBFL needs to be improved by differentiating the contribution of the tests to produce a broader program spectra . We can use coverage metrics of failed tests with respect to the methods of resource-related classes. For example, following the same logic as in the study of Zhang et al. , a failed test that covers fewer methods would be more helpful for locating faults than a failed test that covers more methods.
As we answered RQ3, we found an influence of resource settings on the suspiciousness score. Using the example of Figure 3 presented in Section 2, we argue that resource-interactions can be caused by multiple faulty elements as can be seen by the provided correction of the issue 10. Therefore, similar to the study of Zou et al. , we assume that if a failure is caused by multiple faulty elements, a fault will be localized by SBLF if one faulty element is localized. That is, if SBFL indicates one of the faulty elements, the developer can infer the other faulty elements.
We observed that the SBFL is thus a fault localization technique that can be applied in the context of mobile application testing (RQ1). We see two directions to improve the SBFL. The first is to adapt the ranking metrics to leverage the characteristics of mobile applications, for instance, exploring coverage metrics that estimate the use of the resources (e.g., to point out the methods that effectively use the resources). The second is to get feedback from the ranking process to evolve the test suites. For example, creating more tests to improve the coverage of the methods of resource-related classes.
The interplay of software testing and debugging is a well-known demand for software quality assurance . Wang et al.  emphasized the rule of test artifacts as a support mechanism for debugging. In addition, the adoption of the automation of mobile applications testing by developers is influenced by the generation of test cases that improve debugging and traceability between test cases and features .
As we discussed in Section 2, resource-interaction failures have only recently been explored in mobile applications testing. The characterization of the faults behind these failures is an identified demand . In this way, an improved SBFL approach focusing on these faults could be integrated into a toolset to promote quality-related development activities.
# 6 THREATS TO VALIDITY
We carefully designed and conducted our study. Nevertheless, some threats to validity may have harmed our study results and discussions. We discuss below some major threats and their respective treatments. We divided into construct, internal, external, and conclusion, well-known categories of validity threats .
# Construct validity
In our study, the first threat to construct validity is related to the choice of the subject applications and metrics. We opt to select 8 applications to favor external validity as we discuss below. The use of Ochiai coefficient for measuring the suspiciousness score may not properly capture how SBFL perform in mobile applications. However, we believe that the Ochiai have promise results since it is among the metrics with the best performance .
Another threat concerns the use of artificial faults from a mutation tool. Although some studies have shown limitations in using artificial defects in experiments comparing SBFL techniques , the use of datasets with real defects also has limitations . Our study has a different scope because we do not intend to compare the performance of the SBFL techniques, but rather to look for evidence of the feasibility of using SBFL in the context of mobile applications.
# Internal validity
We use only three mutation operators. Despite the existence of recent studies discussing specialized mutation operators for mobile applications , we believe that the mutation operators used in this study are a representative subset of the traditional five-operator set . Moreover, traditional operators are associated with a significant proportion of real-world application failures as evidenced by the study of Escobar-Velasquez and others .
# External validity
We conducted our study using 8 open-source Android applications. We believe that these applications are representative of the target population for the experimental study, as they were randomly sampled from GitHub public repositories. To mitigate the impact of the representativeness of the Android applications selected for our study, we choose applications from different categories, sizes, and test suite size (ranging from 525 LOC to 3,674 LOC). Another threat to our study is the quality of the test suite for the selected applications. Our entire analysis depends on the test suite’s ability to detect failures. For instance, the study of Heiden et al.  suggests that the accuracy of SBFL is affected by the number of failed test cases. To mitigate the effect of an incomplete test suite, we limited our analysis to applications with a test suite of at least 500 LOC. Furthermore, we limited our analysis to mobile applications developed in Java or mixing Java and Kotlin. Therefore, we cannot generalize to other programming languages and frameworks such as Flutter and React Native.
# Conclusion validity
These results reflect our perceptions and interpretations of the metrics collected from the applications after execution of the testing strategies. All authors participated in data analysis and discussions of key findings to reduce bias from any one person’s interpretation. Nevertheless, we believe that one would get similar results using other metrics and tools that quantify similar attributes for the same mobile applications.
10 https://github.com/commons-app/apps-android-commons/pull/1791
# Applying Spectrum-Based Fault Localization to Android Applications
# SBES 2023, September 25–29, 2023, Campo Grande, Brazil
# 7 RELATED WORK
Several studies investigate failures in mobile applications on different aspects, such as exception tracking , automatic debugging , and setting-related defects . However, we investigated mobile application failures through the SBFL to better understand resource-related failures in mobile applications.
Su et al.  extensively studied to track failures through unique exceptions from 2,486 open-source and 3,230 commercial Android applications. Also, they conducted an online survey of 135 professional application developers to understand how developers handle exceptions. While they manually investigated failures reported by unique exceptions, we used SBFL to identify suspicious faulty program elements related to a software failure. Through their survey, they observed that developers use tools for fault detection. However, they still have many limitations, such as insufficient bug detection. Our results indicate that SBFL can rank more than 75% of the faulty code in 6 out of 8 applications. In addition, they demonstrate that manually tracking failures is a very costly activity. However, with our exploratory study, it was possible to identify that using SBFL for mobile applications is a viable and effective strategy.
Win et al.  describe that debugging and testing Android applications is more challenging than traditional Java programs. In order to reduce costs with testing and debugging Android applications, their work proposes an automatic debugging technique for Android applications called "Event-aware Precise Dynamic Slicing" (EPDS). This technique is based on "dynamic slicing," which is a technique that reduces the scope of program execution to a relevant subset of instructions. Their experiment evaluated the performance of EPDS compared to other techniques for automatically debugging Android applications. Their results showed that EPDS could significantly reduce program scope compared to the other techniques, making it easier for software developers to identify and correct application errors. While they are concerned with reducing the program’s scope, we focus on observing the interactions of resources and using SBFL. We seek to indicate the part of the code that is more prone to failure. Furthermore, this indicates that parts are more prone to failures and thus indicates parts of the code where the tests should be concentrated. Our results indicate that the SBFL can perceive resource interaction failures and artificially inserted failures.
Sun et al.  proposes an approach to identify and correct defects in Android applications related to system settings, they addressed a problem similar to the one we investigated. In their work, system settings can include screen orientation, screen brightness, and system language, among other settings that can be defined by the user (enabling or disabling) or during the usage of applications. While in our work, we also use resources such as Wi-Fi, Bluetooth, and Location. Their work describes an experiment to evaluate the proposed approach’s effectiveness in identifying defects related to system settings in Android applications. Their results showed that the proposed approach could identify several defects not found by other testing techniques, making it a valuable tool for Android application developers. In our work, we insert artificial faults and use instrumentation to activate and deactivate resources through informed configuration.
Marinho et al.  evaluated five sampling strategies in the context of resource-related failures of mobile applications. They generated and analyzed settings for the selected sampling testing strategies: Random, One-Enabled, One-Disabled, Most-Enabled-Disabled, and Pairwise. They observed that the Random strategy found better results concerning settings with failures. We also did a study to understand failures in mobile applications considering the 14 resources used in their work. In addition, we focused on analyzing SBFL to better understand resource-related failures in mobile applications. Finally, they commented on the challenges and difficulties in testing mobile applications concerning verifying failures arising from resource interactions. For example, they observed some challenges: the need for tooling support, the instrumentation of the test suite, and the time needed to test different settings. In order to overcome these challenges, we use SBFL as a technique that can support and reduce testing costs for mobile applications while considering the resources related to failures.
# 8 CONCLUSION
In this paper, we evaluated the use of SBFL aiming to locate faults in 8 Android applications and evaluate the sensitivity to resource interactions. We used faults seeded from a subset of mutation operators aiming to conduct the experimental study. Moreover, we used the Ochiai coefficient for calculating the suspiciousness score.
As a result, SBFL is able to rank more than 75% of the faulty code in 6 out of 8 applications. We found a major influence of resource settings on the suspiciousness score. That is, for the same failure (i.e., mutant), the ranking of suspicious methods varies depending on the combination of enabled resources (e.g., Wi-Fi and GPS). Therefore, we believe that SBFL is a promising technique that should be used in further studies to characterize the faults behind resource interaction failures.
As future work, we suggest the expansion of the experimental study to include applications implemented in other languages and frameworks, such as Kotlin, Flutter and React Native. Moreover, we can investigate specific mutation operators of mobile applications . Another direction is the use of fault localization families  and empirical studies of the SBFL using real faults .
# AVAILABILITY OF ARTIFACTS
We make our data publicly available for further investigations on a GitHub repository 11.
# SBES 2023, September 25–29, 2023, Campo Grande, Brazil