# 2020 IEEE/ACM 42nd International Conference on Software Engineering Workshops (ICSEW)
# Automatic repair of OWASP Top 10 security vulnerabilities: A survey
Alexander Marchand-Melsom ∗
alexamar@stud.ntnu.no
Norwegian University of Science and Technology
Trondheim, Norway
Duong Bao Nguyen Mai∗
dbmai@stud.ntnu.no
Norwegian University of Science and Technology
Trondheim, Norway
# ABSTRACT
Current work on automatic program repair has not focused on actually prevalent vulnerabilities in web applications, such as described in the OWASP Top 10 categories, leading to a scarcely explored field, which in turn leads to a gap between industry needs and research efforts. In order to assess the extent of this gap, we have surveyed and analyzed the literature on fully automatic source-code manipulating program repair of OWASP Top 10 vulnerabilities, as well as their corresponding test suites. We find that there is a significant gap in the coverage of the OWASP Top 10 vulnerabilities, and that the test suites used to test the analyzed approaches are highly inadequate. Few approaches cover multiple OWASP Top 10 vulnerabilities, and there is no combination of existing test suites that achieves a total coverage of OWASP Top 10.
Adequate test suites being crucial to properly benchmark future APR tools, we further analyse the Java test suites used by the surveyed papers to test their approach and two systematic Java test suites (Juliet Test Suite [ 30 ] and Vulnerability Assessment Knowledge Base (VAKB) [ 32 ] [ 34 ]), and establish their coverage of OWASP Top 10.
# CCS CONCEPTS
• Security and privacy → Web application security; • General and reference → Surveys and overviews.
# KEYWORDS
OWASP Top 10, automatic program repair, survey
# ACM Reference Format:
Alexander Marchand-Melsom and Duong Bao Nguyen Mai. 2020. Automatic repair of OWASP Top 10 security vulnerabilities: A survey. In IEEE/ACM 42nd International Conference on Software Engineering Workshops (ICSEW’20), May 23–29, 2020, Seoul, Republic of Korea. APR 2020, Seoul, South Korea, 8 pages. https://doi.org/10/3387940
# 1 INTRODUCTION
For more than 20 years, researchers have attempted to create an automatic tool to repair defective programs. While the field of general bug fixing has seen a substantial rise of activity in recent years, there is yet to be a tool that properly addresses security vulnerabilities. Despite efforts at securing software, the fact remains that security vulnerabilities are a significant threat to many organisations, potentially leading to crippling economic losses. Fully automatizing the process of finding and repairing security vulnerabilities would be a significant step towards a more secure digital world.
This paper is organized as follows. Section 2 presents its related work, while we present our approach in Section 3. The research results are presented in Section 4. We discuss these results and compare them to the related work in Section 5. Finally, we conclude and discuss future work in Section 6.
# 2 RELATED WORK
Liu et al. [ 17 ] systematically reviewed and classified the literature for Test-Based APR tools according to their approach. The authors discussed core issues in research of APR, which include test suite quality, fault localization accuracy, how to generate a patch, and evaluation metrics. They observed that there was a pressing need for a high quality peer-reviewed test suite for APR, but also that the way these tools were evaluated needed to be addressed, as both patch correctness and the generation of a human-readable optimal patch should be weighed in their benchmarking.
Shafiq et al. [ 35] systematically reviewed and classified automatic repair tools. For each solution, they individually identified its strengths.
and weaknesses, and aggregated them according to the used algorithms. The subsequent classification of solutions is first made according to whether they are bug fixing solutions, debugging solutions, or solutions using search algorithms, and then according to six categories: Approaches, Techniques, Tools, Frameworks, Methods, and Systems.
Gazzola et al.  reviewed, classified, and compared techniques from 108 papers about APR. They identified two main approaches to APR tools: Generate-and-validate (G&amp;V) and Semantics-driven. Kong et al.  reviewed five APR techniques: GenProg, RSRepair, AE, Kali, and a brute-force based technique. They evaluated these techniques on a test suite composed of 17 programs of varying size, containing a total of 180 defects. The brute-force technique fared best for small sized programs, but none of the surveyed tools performed well on large test programs, with all tools suffering a significant performance drop. The authors conclude on this by stating that none of the surveyed techniques are ready to be used on large real-world programs.
Durieux et al.  tested 11 Java test-suite-based repair tools on five different benchmarks. They found that while the tools’ performance varied greatly from benchmark to benchmark, they all were able to generate a substantially more significant amount of patches for the Defects4J benchmark . The authors suggested this may be indicative of serious benchmark overfitting, and pointed out that in order to mitigate this problem, APR tools should be evaluated on diverse benchmarks.
Khalilian et al.  evaluated three APR techniques. In an effort to mitigate the absence of standard common benchmarks and provide a comparison framework for APR tools, the authors first compared the surveyed tools according to a set of criteria, then classified them according to a five-level maturity model they created.
Qi et al.  systematically surveyed recent research on APR, and evaluated their evaluation metrics. They isolated 12 main evaluation metrics, categorizing them further into two classes: repair effectiveness and repair efficiency. These findings point to the problem that there currently is no consensus on what metrics should be used to measure a tool’s performance. In light of this, the authors suggest metrics that they view should be included in any evaluation.
Monperrus  compiled a bibliography of existing publications in APR, and categorised them according to their approach. A continuously updated version of this bibliography has also been made available by the same author .
# 3 RESEARCH IMPLEMENTATION AND DESIGN
Existing surveys on APR lack a focus on security bugs in general and OWASP Top 10 vulnerabilities in particular. Furthermore, few surveys question the reliability of the reported performance of APR approaches. This leads to a great uncertainty on what is currently the state-of-the-art and makes it more difficult to identify successful approaches which could be extended. It is also unclear how APR approaches for software security should be tested, and whether existing test suites used to test approaches are adequate, which is problematic for useful comparisons of APR approaches.
Our aim in this survey is to shed light on these issues and attempt to provide an overview over the state-of-the-art, and which test suites future APR approaches for vulnerabilities should use. Our scope will be defined by the following inclusion criteria for the approach described by each publication:
- The approach needs to address a vulnerability found in one or several OWASP Top 10 categories;
- The approach needs to be fully automatic, i.e. it should not require any interaction with a human being beyond its launch;
- The approach needs to effectuate its repair by modifying the source code.
We use the following inclusion criteria for the selection of systematic test suites:
- The test suite needs to be systematic, i.e. it should have set procedures for inclusion or exclusion of new test cases;
- The test suite needs to contain test cases in Java;
- The test suite needs to contain test cases relating to security vulnerabilities.
We chose to limit our scope to Java for systematic test suites because of its prevalence in security-critical applications such as financial services.
# 3 Research Questions
The following research questions were formulated to address these issues:
- RQ1: How can the automatic repair methods reported in the literature be classified according to OWASP Top 10?
- RQ2: How reliable is each method based on their associated test suite and reported performance?
- RQ3: How well-suited are the Java test suites for testing automatic program repair tools?
# 3 Selection of papers and classification
Based on the inclusion criteria mentioned above, we will select papers by searching through six databases or search engines: Springer Link , Oria , IEEE Xplore , ACM Digital Library , Engineering Village , and Google Scholar . These were chosen based on their prevalence in existing surveys and their accessibility. For each database, we will use 120 search queries crafted from the names of OWASP Top 10 vulnerabilities. For each search, we will only examine the first 100 results, as some search engines produce several thousand results, which are often not relevant to the search itself. This means that we expect the maximum amount of results to be 72,000. Due to this amount, each author will go through half of the databases. For each result, we will manually assess its relevance by reading its title and abstract. Then we will read in their entirety results found to be relevant in the previous step. Finally, results passing the previous step will be cross-examined by both authors. The classification according to OWASP Top 10 will be based on the claimed coverage of the analysed publications.
# 3 Reported performance and reliability analysis
The metrics used in the reported performance of APR tools may vary greatly from one publication to another. However, drawing
# 3 Test suite analysis
For each Java test suite, we will use the following metrics based on the observations made in :
- LOC: the total number of Lines of Code in a test suite, which affects a tool’s time performance and capacity to handle significant loads;
- Number of test cases: ideally, a test suite should contain many test cases, in order to present the most diverse testing environment;
- Size difference of test cases: test suites should have test cases of varying size;
- Type of code: if a test case consists of natural code (i.e. the test case comes from a real full-sized program), it will present a more realistic setting, but it will be more difficult to assess how many vulnerabilities are present in it, and their details (which vulnerability class, what the most appropriate patch is...). This makes assessing a tool’s correctness and completeness more difficult. Conversely, if a test case consists of synthetic code (i.e. the test case was written for the express purpose of being used in a test suite), all details about the vulnerabilities present in the test case will be known. However, the setting will be less realistic, and such test cases also tend to be smaller. An ideal test suite would contain a combination of both types of code;
- Availability: the test suite should be easily and openly available for researchers, in order to promote usage and peer-review;
- Vulnerability classes: the test suite should cover most possible classes of vulnerabilities. In this analysis we will look at which OWASP Top 10 classes each test suite covers;
# 4 RESEARCH RESULTS
# 4 RQ1
The results of our search are presented in Table 1, which shows the number of results for each database or search engine. The total number of results amounts to about half of the expected number of results.
From these 38,095 results, 27 were admitted to the cross-examination phase. During this phase, we found one duplicate, one publication that relied on user interaction to repair its target, and five papers that implemented so-called runtime approaches which do not modify the source code. All these publications were removed, which leaves 20 publications that were inside the scope we had defined. While we unfortunately did not take note of which papers were rejected before the cross-examination, they were largely similar both in nature and in proportion as the papers rejected in the cross-examination phase. These are summarised and classified according to the OWASP Top 10 categories they cover and their
target language in Table 2. What is immediately noticeable is that there are only two target languages, namely Java and PHP, despite the omnipresence of JavaScript in web development. The OWASP Top 10 coverage is summarised in Figure 1, which underscores the strikingly sparse coverage of OWASP Top 10 categories. The pre-dominant categories are very clearly A1 - Injection and A7 - XSS, with 14 and 10 approaches targeting them respectively, followed by A3 - Sensitive Data Exposure with 6 approaches. The other OWASP Top 10 categories are scarcely evaluated, and half of them are not evaluated at all. This may be in part due to the fact that most of the targeted vulnerabilities can be solved with proper input sanitization, which is a repair that is heavily standardised and can often be implemented by inserting a single line of code, and is hence easier to automatize.
# 4 RQ2
The reported performances of the surveyed papers are summarised in Table 3. P8 presented two distinct approaches, which were labeled P8-1 and P8-2. Some papers (such as P11) tested their approach on distinct test suites with different metrics, hence their reported performance is summarised individually for each test suite. We may immediately see that two papers (P19 and P20) did not test their approach, making any analysis of their effectiveness very difficult. Also difficult to analyse are P17 and P14, due to their lack of reported performance metrics, or vagueness in their report ("All" patches were true positives in the case of P17, which, without the amount of generated patches, does not provide much information). P2 suffers from a very small test suite of only 250 estimated LOC, made solely for the purpose of testing this approach, which increases the likelihood of test suite overfitting. It is difficult to determine its aptitude to repair vulnerabilities outside this test suite, a weakness recognised by the authors. The same problem is present in the case of P8-1, with a test suite of only 488 LOC.
Although its test suite is better than P8-1, and its number of generated patches is good, P8-2 does not report whether any of these patches were correct, which weakens its reliability. While having a test suite much larger than P8-1, P1 still suffers from the fact that its test suite is composed of a single test case, which is not enough to emulate the varied environment that is real-world vulnerability repair, and increases the probability of test suite overfitting. P9 presents the same problem, with the added issue of not testing whether the patches break any functionality, although its performance results remain impressive. P18 uses the same general technique (i.e. generating PreparedStatements) as P9, but is tested on a larger test suite composed of 9 test cases, which would indicate a greater reliability. Its biggest weakness is its omission of the time metric. Also generating PreparedStatements, P16 has been tested on an even larger test suite, but containing only 4 test cases. Interestingly, while the test suite is larger, the amount of vulnerabilities detected and patched is much lower in P16 than in P18 or P19, suggesting either that P16 is not as proficient, or that the test suites used by the two aforementioned papers were exceptionally vulnerable.
P5 uses a decently-sized test suite with 5 test cases of varying size, however the approach is non-deterministic and its success rate is not reported. Additionally, the time metric is not reported despite the use of a Genetic Algorithm, a method that may potentially take a substantial amount of time. It is uncertain then whether the reported performance would hold in a different setting. The same test suite as P5 is used by P6, which presents a deterministic method with a slightly better performance, although with a high number of detected, but not repaired, vulnerabilities. P7 uses a similar test suite as P6, but it is difficult to assess its performance due to the use of number of vulnerable files instead of number of vulnerabilities, which makes the report more imprecise.
A large test suite with 10 test cases was used by P3, however it suffers from a relatively high number of false positives, and uses an inordinate amount of time for a test suite of this size. P15 uses a very well thought out approach to test its tool, validating patches both manually and through the help of the developers of the vulnerable applications. Its test suite is also extremely large, with 8,640 test cases. However, it is not reported how many vulnerabilities were found in each application, nor how large the apps were, which makes it harder to compare with other methods. The test suite used by P4 is one of the largest in this survey, and contains 6 different test cases of varying size. The paper reports an important amount of detected vulnerabilities, however only less than half of these are repaired, albeit with no false positives.
P10, P11, P12, and P13 all relate to the same tool, WAP, with each paper introducing improvements on the previous one. The tool is thoroughly tested with several huge test suites containing both synthetic and natural code, and a large amount of test cases of varying size. In terms of reliability, the only aspect that may be commented on, is the suspiciously low amount of false positives considering the amount of false positives in the real world, which is a trend we observe throughout the surveyed papers. It is interesting to note that most test suites in this survey are ad-hoc and with natural code: about 70% of test cases are made of open source web applications, and only two papers use formal test suites. Barely 8% of test cases used synthetic code, and most in the form of purposefully vulnerable web applications.
# 4 RQ3
None of the 20 test cases contained within the 5 Java test suites used in the surveyed papers were found, most links having expired and surviving versions of programs being different from the ones used. P15’s 8,640 apps were not found either. Hence, we only analyse the two systematic test suites.
Juliet and VAKB are strikingly different (see Table 4): VAKB dwarfs Juliet in terms of total number of LOC, but contains much less test cases, with 164 of them even being unavailable. The size difference between the smallest and largest test case is also very different: Juliet’s largest test case is about 84 times larger than its smallest, while VABK’s largest test case is more than 5,800 times larger than its smallest. This is explained by the fact that Juliet is entirely composed of synthetic code, while VAKB is entirely composed of natural code. Yet, they complete each-other in their coverage of OWASP Top 10 categories, and being of different types of code, are intended for different types of testing loads. It should be noted that we could not find the repository or fix commit for 164 test cases in VAKB.
"Unreported" indicates that a metric has not been reported by the paper, and "N/A" means that the paper has not tested its approach. The "False positives", when not reported, were calculated by subtracting the number of true positives from the number of generated patches.
The number of LOC per OWASP category for each test suite seems to lack test cases for the other OWASP Top 10 categories. Figure 2 emphasises the massive difference in size between the test suites, but also indicates that they have a very different focus, with Juliet being more focused on A1, and VAKB on A5. It should be noted that A9 does not have any corresponding CWEs, and is hence omitted.
This is further reflected in the number of test cases per OWASP category (Figure 3), where Juliet also seems to emphasize A7. However, it also shows that Juliet heavily emphasises A1 and A7, and VAKB seems to have a better coverage of each OWASP category individually.
# 5 DISCUSSION
# 5 Comparison with related work
Shafiq et al. effectuated a survey like us, although we restricted our scope to APR of vulnerabilities, and to source code modification. This increased focus allowed us to identify 19 approaches that Shafiq et al. had not classified, despite 18 of them being published before their literature review, and being inside their acceptance criteria. Thus, all but one approach we surveyed had not been analyzed or categorised by Shafiq et al. We also performed a deeper analysis of each approach, as we looked at their performance in the context of their test suite, which we also analyzed.
The approach of King et al. is fundamentally different from our approach, as they focused on benchmarking the tools they surveyed on a common test suite, while we compared tools by analyzing their reported performance and test suite. Our approach allowed for more tools to be analysed, with the additional analysis of test suites giving greater insights into how future APR tools should be tested. A good basis of test suites is present in the form of Juliet and VAKB, which could be extended to cover the missing OWASP Top 10 categories. Regardless of the test suite used, it should be easily and durably available for other researchers to analyse and use, as most test suites found in the benchmarking efforts of Durieux et al. gave interesting insight into the potential widespread test suite overfitting in APR, and possible mitigation strategies. Our results seem to indicate a similar tendency in APR of vulnerabilities, seeing that almost all of the test suites used to assess the performance of approaches we analyzed were of low quality and arbitrarily chosen, which might be an indication of test suite overfitting.
# 6 CONCLUSION AND FUTURE WORK
This survey reveals a very uneven coverage of OWASP Top 10 in APR, with categories such as A1 and A7 being the subject of multiple papers, while other categories are not even mentioned. There is room for improvement in this respect, and future work should probably focus on other categories than A1 or A7. It is also interesting to note that many papers rejected from this survey implemented runtime APR, i.e. they did not modify source code. Our work shows that few researchers adequately test their approaches, which corroborates previous research that pointed to a lack of standardised performance assessment. A standard performance measure approach and a well-designed standardised test suite would greatly facilitate comparisons between tools.