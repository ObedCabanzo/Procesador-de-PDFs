# Exposed! A case study on the vulnerability-proneness of Google Play Apps
Andrea Di Sorbo1 · Sebastiano Panichella2
Accepted: 13 May 2021/ Published online: 8 June 2021
© The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2021
# Abstract
Mobile applications are used for accomplishing everyday life activities, such as shopping, banking, and social communications. To leverage the features of mobile apps, users often need to share sensitive information. However, recent research demonstrated that most of such apps present critical security and privacy defects. In this context, we define as vulnerability-proneness the risk level(s) that users meet in downloading specific apps, to better understand whether (1) users select apps with lower risk levels and if (2) vulnerability-proneness of an app might affect its success. We use as proxy to measure such risk level the “number of different types of potential security issues exhibited by the app”. We conjecture that the vulnerability-proneness levels may vary based on (i) the types of data handled by the app, and (ii) the operations for which the app is supposed to be used. Hence, we investigate how the vulnerability-proneness of apps varies when observing (i) different app categories, and (ii) apps with different success levels. Finally, to increase the awareness of both users and developers on the vulnerability-proneness of apps, we evaluate the extent to which contextual information provided by the app market can be exploited to estimate the vulnerability-proneness levels of mobile apps. Results of our study show that apps in the Medical category exhibit the lowest levels of vulnerability-proneness. Besides, while no strong relations between vulnerability-proneness and average rating are observed, apps with a higher number of downloads tend to have higher vulnerability-proneness levels, but lower vulnerability-proneness density. Finally, we found that apps’ contextual information can be used to predict, in the early stages, the vulnerability-proneness levels of mobile apps.
# Keywords
Mobile applications · Vulnerability-proneness · Empirical study
# Communicated by
Denys Poshyvanyk
# Authors
Andrea Di Sorbo
disorbo@unisannio.it
Sebastiano Panichella
panc@zhaw.ch
# Affiliations
1 Department of Engineering, University of Sannio, Benevento, Italy
2 Zurich University of Applied Science, Zurich, Switzerland
# 1 Introduction
Nowadays, mobile applications are widely adopted in most of our everyday activities. About 2 and 2 million apps are currently available for download on Google Play Store and Apple App Store, respectively,1 with worldwide users relying on mobile applications to deal with a lot of professional and personal tasks, such as shopping, banking, social communication, and events organization . However, as reported by Gartner, a well-known market research organization, several popular apps typically process a lot of sensitive data (e.g., user’s location, lists of contacts, personal photos, etc.), providing often a little or no security assurances  due to either inappropriate implementations or poor design choices .
Previous studies demonstrated that the users’ perception, satisfaction, and judgments of mobile apps is influenced by many factors related to app development, such as (i) the change- and fault-proneness of adopted APIs , (ii) the complexity of users interfaces , (iii) the app size , and (iv) the amount of promotional images on the web-store pages (that could represent a proxy for estimating the amount of different functionalities provided by the app) . Complementary to what reported also by Gartner, recent research (Taylor and Martinovic 2017a; Aliasgari et al. 2018), conducted on popular apps handling sensitive user data (i.e., Financial and Health apps), demonstrated that the vast majority of mobile apps tend to suffer from security and/or privacy issues.
In general, consumers are less likely to rely on products experiencing privacy flaws . However, from a practical point of view it is not clear the extent to which (i) the users are aware of the risks they tackle while using insecure mobile applications  and (ii) the extent to which vulnerability risk level(s) of a mobile app can hinder its success. We argue that users tend to be unaware of the risks concerning downloading and using apps of different categories, and this assumption is also based on the fact that mobile apps generally receive few user reviews related to security and privacy concerns . Moreover, app details shared in app stores rarely provide (or do not provide) information about the “security risks associated to the usage of certain apps” .
In this context, we define as vulnerability-proneness the risk level(s) that users meet in downloading specific apps. Thus, we consider, in this paper, as proxy to measure the vulnerability-proneness of an app, “the set of the different types of potential security defects exhibited by the app”. We argue that higher vulnerability-proneness levels may result in a higher probability (or potential) of an app of being attacked, as a larger attack surface (i.e., the set of different ways in which an adversary might cause damage Manadhata and Wing, 2011) is exposed. This means that, vulnerability-proneness measures the range of different opportunities malicious users can attempt to exploit for triggering an attack. More specifically, rather than summarizing the proportion of flawed code units, vulnerability-proneness is more oriented at modeling the attack surface potential hackers have, by only quantifying vulnerabilities of different natures that could be likely exploited. For instance, given a specific vulnerability type, Vt, vulnerability-proneness does not report how many times vulnerabilities of the Vt type were found in the app’s code, but it only accounts that the app is vulnerable to the specific kind.
1 https://buildfire.com/app-statistics
Empir Software Eng (2021) 26: 78
of weakness Vt. In this paper, we propose to answer the following and rising questions concerning the vulnerability-proneness of app that, to the best of our knowledge, no prior work investigated:
- Do users actually select apps exhibiting low security risks? What are the existing (if any) relationships among (in)security issues and app success (i.e., rating and downloads)?
Mobile app vulnerabilities have been explored from various research perspectives. For example, research literature proposed (i) comprehensive analyses of specific vulnerability types (Chin et al. 2011; Clark and van Oorschot 2013; Zhou and Jiang 2013), (ii) approaches and tools for automatically detecting security flaws , (iii) enhancements to current security models , and (vi) investigations on app vulnerabilities evolution (Gao et al. 2019; Taylor and Martinovic 2017b). Nevertheless, little attention has been paid to better understand how and to what extent security risks can affect the success of mobile apps. To fill this gap, this paper answers the aforementioned general questions, by proposing the following contributions:
- we first study (i) the most widespread vulnerability warnings exhibited by about 1,000 apps mined from the official Google app market, and (ii) the diffusion of specific vulnerability warning types over different app categories;
- we investigate the extent to which the vulnerability-proneness influences app success. Previous work  demonstrated that successful apps have more dependence on libraries. As third party code may represent the main carrier of app vulnerabilities , we aim at understanding whether apps with higher success are more or less prone to security flaws;
- we evaluate the extent to which an app’s contextual information (e.g., app metadata, such as app category, permissions required, etc.) can be used for automatically identifying, in early stages, its vulnerability-proneness, with the benefit of estimating the security and privacy concern levels, without the need of downloading or inspecting the actual app executables or sources.
Our investigation is conducted according to the overarching hypothesis that user perception of app (in)security levels may vary based on (i) the types of data handled by the app, as well as (ii) the app usage scenario. For instance, users may be more concerned to use an insecure app to access their bank account rather than sharing their position when using a flawed navigator app. We argue that these factors strictly depend on the actual app category, as apps belonging to different categories deal with different users’ data and present different user diffusion/popularity levels.
# Paper structure
The paper proceeds as follows: Section 2 analyzes the related literature, while Section 3 discusses the empirical study design. Section 4 presents the main findings achieved, while the threats that could affect our results are discussed in Section 5. Finally, Section 6 draws the conclusions and outlines future research directions.
# 2 Related Work
In this section, we focus the discussion on two main research fields from the literature: (i) studies exploring vulnerabilities in mobile apps, and (ii) studies investigating the factors that could influence the app success.
# 2 Vulnerabilities in mobile apps
In the mobile domain, software vulnerabilities have been extensively studied in prior research. Most of these studies were aimed at (i) identifying new types of flaws that could expose mobile users to security risks, and (ii) proposing approaches or tools for detecting such kinds of flaws. As a result, a non-exhaustive list of some of these issues targeted (i) inter-application communication vulnerabilities , (ii) misuses of SSL/TLS protocols , (iii) component hijacking vulnerabilities , (iv) security risks related to file browsing (Wu and Chang 2014), (v) WebView vulnerabilities (Chin and Wagner 2013), (vi) security flaws related to input validation , and the major types of privacy policy violations .
Besides, several empirical investigations concerning different aspects related to the vulnerabilities occurring in mobile apps and systems have been carried out. In this context, Jimenez et al. (2016) studied Android vulnerabilities reported in the National Vulnerability Database (NVD) between 2008 and 2014 and characterized the corresponding fixes. Linares Vásquez et al. (2017) conducted a large-scale empirical study on Android OS-related vulnerabilities aimed at (i) characterizing the types of Android-related vulnerabilities, (ii) exploring the layers and subsystems from the Android OS affected by vulnerabilities, and (iii) investigating the survivability of vulnerabilities, as the number of days between the vulnerability introduction and its fixing. Similarly, Thomas et al. (2015a) explored API vulnerabilities in Android and quantified the rate at which these flaws are fixed on real devices. In a subsequent work, Thomas et al. (2015b) studied the different players in the Android ecosystem who must collaborate to provide updates aimed at fixing security flaws and showed that the bottleneck is represented by the manufacturers, who often fail to provide updates to fix critical vulnerabilities. More recently, Watanabe et al. (2017) used automated vulnerability scanners to conduct a large-scale study aimed at understanding whether app vulnerabilities depend on software libraries and revealed that approximately 70% of vulnerabilities of free apps and 50% of vulnerabilities of paid apps stem from software libraries, particularly from third-party libraries. Also, Gao et al. (2019) relied on vulnerability-finding tools and observed for the first time certain security flaws (e.g. vulnerability reintroduction) that are strictly related to the app’s evolutionary perspective.
Similarly to prior research we use a state-of-the-art tool for detecting potential vulnerabilities in Android applications concerning, among others, (i) SSL-related issues, (ii) sandbox mechanism, and (iii) code injection.
# 2 Factors affecting app success
Several studies have investigated the relationships between the rating of an app and different app characteristics (Bavota et al. 2015; Chia et al. 2012; Harman et al. 2012; Ruiz et al. 2014; Taba et al. 2014; Tian et al. 2015; Vásquez et al. 2013; Guerrouj et al. 2015; Corral and Fronza 2015; Khalid et al. 2016). For example, Harman et al. (2012) examined the existing relations between description, download count, and average user rating of BlackBerry apps, while Tian et al. (2015) have demonstrated that marketing effort, app size and the target SDK version of Android apps are the most relevant factors impacting their success. Complementary to these works, recent studies investigated the reasons behind the removal of Android apps from Google Play , and demonstrated that several security and privacy approaches do not take advantage of the whole range of features of an appified.
ecosystem, which could be exploited to design mechanisms less disruptively , as the one proposed in our paper.
Some studies explored the types of software libraries used in mobile apps that can impact user satisfaction. In this context, Linares Vásquez et al. (2013) and Bavota et al. (2015) demonstrated that change- and fault-proneness of APIs used in apps could represent a threat to mobile app success. Other researchers  have analyzed the impact of ad libraries on app ratings. On the one hand, they found no evidence that the number of ad libraries in apps is related to their rating. On the other hand, they demonstrated that the integration of certain ad libraries can negatively affect app rating.
Further research efforts focused on analyzing features specifically related to app development and source code quality. Corral and Fronza (2015) showed that source code metrics only marginally relate to market success. Taba et al. (2014) explored the relationships between the complexity of user interfaces used in Android apps and the user-perceived quality, measured by the number of downloads and app rating. They found that simpler user interfaces may result in a better perceived quality by users.
A close work to ours is the one by Chia et al. (2012), who demonstrated that community ratings used in app markets are not reliable indicators of privacy risks of an app. In particular, they quantified privacy risks in terms of permissions requested by an app and found that popular Android applications typically tend to request more permissions than new apps, while no negative relations have been found between the rating of an app and the number of permissions requested.
Consistently to these previous studies, we estimate the app success in terms of app average rating and the number of app downloads. In addition, as done in previous work , we also focus on investigating whether average app rating is reflected in the level of security and privacy risks of apps. Differently from prior work, we (i) define the vulnerability-proneness of an app as the number of different types of potential security flaws (and, thus, that attackers could exploit) exposed by the app, (ii) study the different vulnerability-proneness levels exposed by apps belonging to different categories, (iii) explore the eventual relationships that could exist between the vulnerability-proneness of an app and its success, and, finally, (iv) investigate the extent to which an app’s contextual information could be used to predict its vulnerability-proneness level. Concerning the last point, Tao et al. (2020) recently proposed to identify security issues for mobile applications based on user review summarization. Differently from this work, we use more structured app store metadata to achieve our goal. App store metadata is information the users typically consult before downloading and installing an app, providing information about app functionalities or permissions that the app requires. We believe that this information could be useful for estimating security risks related to the download and usage of an app. However, for future work, we envision the usage of both sources of information to improve the accuracy of our approach.
# 3 Study Design
The goal of this study is two-fold: (i) to assess the vulnerability-proneness of mobile apps belonging to different categories from the official Google Play app store, as well as (ii) to estimate the extent to which vulnerability-proneness can affect app success. To pursue this goal, we formulated three research questions:
# Empir Software Eng (2021) 26: 78
# Research Questions
- RQ1: Which are the different vulnerabilities exhibited by Google market apps belonging to different app categories? The first research question has the purpose of studying (i) whether different categories of apps expose different types of vulnerability warnings, and (ii) if the vulnerability-proneness levels of apps generally vary according to app categories. We conjecture that the types of vulnerabilities exposed by an app can be somehow related to the functionalities the app provides. Indeed, (i) many vulnerabilities stem from software libraries , and (ii) developers can use similar APIs for implementing similar app functionalities (Vásquez et al. 2016). Thus we use the standard Google Play app categories to verify our conjecture, as these categories could be indicative of the functionalities provided by a software application (Vásquez et al. 2014).
- RQ2: Does the vulnerability-proneness of Google market apps affect app success? This research question aims at investigating if Google Play apps having lower market success (i.e., lower ratings and/or lower number of downloads) are more vulnerability-prone. The underlying hypothesis is that a major amount of vulnerabilities may increase the probability of attacks and user experiencing attacks would provide low ratings or negative posts/feedback to discourage other users from installing the app, thus impacting the apps popularity.
- RQ3: Is it possible to predict the level of vulnerability-proneness of an app by using the app’s contextual information? This research question is aimed at understanding whether (or not) the contextual information that users examine before installing an app is useful (or not) for estimating its level of vulnerability-proneness.
# 3 Context selection and Dataset construction
To answer our research questions, we collected a dataset comprising 1,002 mobile applications downloaded from the Google Play market. We focused our attention on free apps for practical reasons (paid apps would clearly require a fee) and decided to target Google Play as users of devices running Android OS generally choose their apps from this app store  that represents the official Android market. It is worth pointing out that in several prior studies  investigating Android vulnerabilities and security defects, the number of samples analyzed is similar to ours.
# The RICO mobile apps dataset & Sampling strategy
To guarantee a non-biased selection of the data, we relied on information collected in the RICO mobile apps dataset , which contains metadata information of more than 9k Android apps spanning 27 different app categories. Although there are other (and larger) datasets of apps (e.g., Androzoo Allix et al. 2016), we decided to consider the RICO dataset for 3 main motivations:
- Real crowd-workers successfully interacted with each app contained in this dataset; this reduces the likelihood of considering toy apps ;
- About 30% of the apps in this dataset have been removed from the Google Play; this allows to reduce the likelihood of considering apps that have been already judged as low-quality, risky, or malicious by the app market itself ;
– all the apps in this dataset were released in 2017, at the latest. As vulnerabilities may survive a long time after the first release of an app , apps with years of development behind them are more likely to implement well-established processes for mitigating security threats.
Since several apps (i.e., about 30%) appearing in the RICO dataset are no longer available for download on the Google Play store, we selected a statistically significant sample from this large dataset, to ensure a reliable and fair representativeness of the data collection (i.e., a confidence level of 95% and a confidence interval less than 3). In particular, we performed a stratified random sampling of apps present in the RICO dataset, by selecting apps belonging to different app categories. The number of apps downloaded for each category, along with the ranges of average star ratings assigned to these apps are shown in Table 1. The categories considered in our dataset are a subset of all Google Play categories.
To have a sample with more balanced data concerning the app ratings, we applied the following steps to select the apps to include in our dataset:
1. We scraped Google Play to collect the updated average rating values associated with the apps in the RICO dataset that are still available on Google Play.
# 2.
For each app category C, we computed the median of average ratings values, Mi, assigned to apps of the RICO dataset belonging to C.
For each app category Ci, a half of the Ci apps included in our sample have an average rating value below or equal to Mi, and the remaining half of the Ci apps in our sample have an average rating value above Mi.
For instance, for the Medical category we selected 33 different apps (as reported in Table 1), 16 of such apps have an average rating value below or equal to 4, while the remaining apps have an average rating value higher than 4. It is worth noticing that in our sample we only considered apps having at least 500 raters, this to reduce the risk that our results may depend on very subjective or small amount of ratings. Consistently to previous work , we adopted this threshold (i.e., 500 raters) to target popular apps.
According to recent statistics2 there are 2 million apps available for download on the Google Play Store. Thus, our dataset is also a representative sample (i.e., a confidence level of 99% and a margin of error of 4%) of all apps present on the Google app market.
# Data Collection
Once selected the apps to download, all APK files were downloaded from Google Play. In particular, we implemented a web crawler able to automatically scrape Google Play with the aim of gathering updated information about the apps from the store and collecting the APK files of the selected apps.
Along with the APKs we collected all the information provided by the Google Play store related to each app. More specifically, we gathered the following metadata: (i) the app’s name, (ii) the app’s description, (iii) the app’s Play store category, (iv) the app’s size, (v) the photos/screenshots appearing in the app’s market webpage, (vi) the app’s average rating, (vii) the number of users who rated the app, (viii) range of the number of installs, (ix) the list of permissions the app requires, and (x) monetization-related information (i.e., whether the app contains ads and/or offers in-app purchases). All the APK files, and the related metadata were collected during December 2019.
The downloaded apps were scanned through AndroBugs3, with the aim of detecting potential security defects related to each of the considered apps. Among the several vulnerability scanning tools available, we selected AndroBugs as, differently from other vulnerability scanners, it tries to simulate the operation of an app considering the paths or means through which such weaknesses would be exploited, instead of just scanning the code for weak spots (Darvish and Husain 2018). Besides checking for many known common vulnerabilities in the Android apps, AndroBugs also inspects the code against (i) missing best practices, and (ii) dangerous shell commands (e.g., su). In particular, AndroBugs is a state-of-the-art well-known vulnerability scanner for mobile applications that was first presented at the BlackHat security conference, and has been used in several previous studies investigating vulnerabilities occurring in mobile apps . This static detection tool was also successfully used to find vulnerabilities and other critical security issues in Android apps developed by several big players, such as: Facebook, eBay, Twitter, etc. .
All the reports generated by AndroBugs have been parsed to enumerate, for each app, the detected vulnerabilities. Listing 1 illustrates an excerpt from an AndroBugs report. To each detected vulnerability a severity score (e.g. Critical in Listing 1) and a type (e.g. Runtime Command Checking in Listing 1) is assigned. It is worth noticing that we only collected
2 https://buildfire.com/app-statistics - accessed on February 2021.
3 https://github.com/AndroBugs/AndroBugs Framework
# Empir Software Eng (2021) 26: 78
# Listing 1  Excerpt from an AndroBugs report
vulnerabilities which are marked as Critical, Warning, or Notice by AndroBugs, as vulnerabilities with a Info categorization indicate that the specific issue was not found on the specific apk . Some of the potential security weaknesses that the AndroBugs static analyzer is able to detect are related to: (i) SSL connections, implementation and certificate validation, (ii) WebView- and Fragment-related vulnerabilities, (iii) implicit intents, (iv) data storage, (v) KeyStore protection, (vi) Android Manifest settings, and (vii) entry points for command injection. The full list of vulnerabilities checked is reported in our online appendix
Replication package We make available in our replication package5 all the raw data used to answer our research research questions.
# 3 Research method
To answer our RQ1 we compared the vulnerability warnings detected in the apps belonging to the different Google Play store’s categories. In particular, to characterize the vulnerability-proneness level of an app category, we group all the apps in our sampled dataset (see Section 3) that belong to that specific category and compute the vulnerability-proneness levels related of these apps. Then, to establish whether there are categories exhibiting higher/lower vulnerability-proneness levels, we compared the vulnerability proneness levels associated with each category. Besides, to check whether the observed differences are statistically significant, we make use of the Kruskal-Wallis test (Kruskal and Wallis 1952) and subsequent Mann-Whitney pairwise comparison  (with the Holm’s p-value correction procedure  and α = 0), for identifying the specific pairs whose differences have statistical evidence. The Kruskal-Wallis test is a rank-based nonparametric test that is used to determine if statistically significant differences between more than two groups of an independent variable can be observed. However, since this test does not allow identifying the specific groups for which the differences have statistical evidence, we perform a posthoc analysis (using Mann-Whitney pairwise comparison) to determine which levels of the independent variable differ from each other level. To counteract the problem of multiple comparisons and reduce the probability of obtaining Type 1 errors (false positives), we use the Holm’s correction procedure, which allows adjusting the rejection criteria for each of the individual hypotheses tested.
In particular, we tested the following null hypothesis:
H01: There are no significant differences between the vulnerability-proneness levels of apps belonging to different Play store’s categories.
4 https://github.com/adisorbo/vulnerability proneness/wiki/Vulnerability-Types
5 https://github.com/adisorbo/vulnerability proneness
We also estimated the magnitude of the differences with statistical significance, through the Cliff’s delta (or d) effect-size measure (Grissom and Kim 2005). Following the guidelines in Grissom and Kim (2005), we interpret the effect-size as: small for |d| &lt; 0, medium for 0 ≤ |d| &lt; 0, and large for |d| ≥ 0. For graphically visualizing the distributions, we make use of boxplots. Moreover, to complement this quantitative analysis, we qualitatively investigated the different types of vulnerability warnings encountered in apps belonging to categories exhibiting higher/lower vulnerability-proneness levels.
# RQ2
To answer RQ2, we compared the vulnerability-proneness levels of apps having different app success, using a methodology similar to the one adopted by Linares (Vásquez et al. 2013). The dependent variable for this research question is represented by the success of the considered apps. For estimating app success, as in previous studies, we make use of two proxy values—i.e., the average rating and the number of downloads—as they could be easily gathered from the Google Play store webpage. The independent variable is the number of potential security defects exhibited by the different apps.
Concerning the rating, we clustered the apps in four different groups based on their average rating values. To achieve this goal and assign each app to one of the groups, we computed the quartiles of the distribution of average rating values assigned to the apps. In particular, given ra the average user rating, the four sets are: (i) group1 = [apps having ra ≤ 1st Quart.]; (ii) group2 = [apps having 1st Quart. < ra ≤ Median]; (iii) group3 = [apps having Median < ra ≤ 3rd Quart.]; and (iv) group4 = [apps having ra > 3rd Quart.]. Similarly, we clustered the apps in three different groups based on their install class, as Google Play does not provide the exact number of downloads for apps. In particular, the three sets are: (i) set1 = [apps having up to 1,000,000 downloads] (i.e., up to 1M); (ii) set2 = [apps having from 1,000,001 to 50,000,000 downloads] (i.e., up to 50M); and (iii) set3 = [apps having more than 50,000,000 downloads] (i.e., more than 50M). We tested the following null hypothesis:
H02: There are no significant differences between the vulnerability-proneness levels of apps belonging to different success groups.
More specifically, we compared the distributions of the vulnerability-proneness levels associated with the different rating groups, and the vulnerability-proneness levels associated with the different download groups, through the Kruskal-Wallis test and subsequent Mann-Whitney pairwise comparison (with the Holm’s p-value correction procedure and α = 0). In addition, we estimated the magnitude of the statistically significant differences, through the Cliff’s delta. To investigate whether specific results could be observed among the different app categories, we repeated the analysis for each of the app categories in our dataset.
We observed that the apps in our dataset strongly vary in size (i.e., from apps having less than 10 classes to apps having tens of thousands of classes). To account for any potential bias that the difference of app sizes may introduce, beyond evaluating the number of cumulative warnings obtained for each app, for both RQ1 and RQ2 we also proceeded to a normalized analysis. Specifically, for each app, we divided the number of vulnerability warnings signaled by AndroBugs by the number of classes. This allows us to estimate the vulnerability-proneness density of each app.
# RQ3
To answer RQ3 we used app-related information collected from the Google Play store to (i) train a set of machine learning (ML) classifiers and (ii) evaluate the extent to which they were able to identify the vulnerability proneness levels of apps. In particular, to have a balanced dataset, after computing the median value, Mvulns, of the distribution of the vulnerability-proneness values associated with the apps in our dataset, we assigned to each
app the label (i) low, if its vulnerability-proneness level was lower (or equal) than Mvulns, or (ii) high, if its vulnerability-proneness level was higher than Mvulns. These labels represented our ground truth for computing the prediction performance. By relying on the Weka tool6, three different ML algorithms—namely the Naive Bayes, J48, and Random Forest—were trained to predict if the vulnerability-proneness level of an app should be marked as either low or high. The selection of these specific algorithms is not random, as they have been successfully adopted in previous work concerning defect prediction tasks (Giger et al. 2012; Kaur and Kaur 2014).
# Extraction and pre-processing of ML features
To train such ML algorithms, we considered 13 features provided by the app market concerning five different aspects that might be correlated with the vulnerability-proneness of apps. The meaning and rationale of each selected feature is described in Table 2. It is worth noticing that all the features reported in Table 2 can be gathered by simply scraping Google Play. Most of these features are of (i) numeric (i.e., Description, Length, Photos, APK Size, Average Rating, Number of Raters and Number of Permissions), (ii) nominal (i.e., Play Store Category and Number of Installs), or (iii) boolean (i.e., Contains Ads, In App Purchase, and each specific Permission in the list) types. Note that, while the relationships between the individual features in Table 2 and security risks have been extensively studied in prior research (Watanabe et al. 2017; Krutz et al. 2016; Felt et al. 2011a; Yeom and Won 2019; Yang et al. 2017), we leverage findings from these previous studies for selecting significant features to train machine learning algorithms.
# Textual features
However, Name and Description features involve text contents that must be properly preprocessed to be used as features to train the ML models. Previous work demonstrated that natural language texts could be profitably treated (through well-known text analysis techniques aimed at extracting relevant textual features from them) and used for issue/vulnerability classification (or prediction) tasks . In our context, for each considered app, the Name and the Description features have been concatenated. The resulting strings have been then used as an information base to build a textual corpus (i.e., bag of words composing the concatenated text elements), that was pre-processed by applying stop-word removal (using the English Standard Stop-word list) and stemming (i.e., English Snowball Stemmer) (Baeza-Yates et al. 1999). The usage of a stemming approach is not random, since it was successfully leveraged to reduce the number of text features for the ML problems in the mobile context . In future work we plan to investigate the effect of using lemmatization approaches in our results. As result of this process, a set of textual features (i.e., words) with relevance values (i.e., the frequency of the words in the app textual corpus) is associated to each app. In addition to these text features, we also computed, for each app, the n-grams— with [n ∈ [2 − 4]]— appearing in the aforementioned textual corpus to preserve word locality information. The rational of using n-grams is due to the fact that app functionalities are usually described by groups of 2,3 or 4 words rather than single words . Thus, each resulting text feature (i.e., word or n-gram) is weighted using the tf-idf weighting score (Baeza-Yates et al. 1999).
6 https://www.cs.waikato.ac.nz/ml/weka/
# Empir Software Eng (2021) 26: 78
# Empir Software Eng (2021) 26: 78
Static analysis features Previous work (Scandariato and Walden 2012) demonstrated that code metrics can be used to predict Android vulnerabilities. Thus, we also used widely-known static analysis tools to extract a set of code-related metrics, for two main reasons:
- to compare our results with a baseline approach and verify whether machine learning models trained with information extracted from the app store could achieve similar results to the ones obtained by measuring code-related features. If similar results would be achieved with the two approaches, using app store metrics as a proxy for estimating the vulnerability-proneness level of an app would be a clear advantage since there would be no need to download and inspect the app’s code;
- to better understand whether app-related details extracted from the Google store could provide complementary information to the one provided by code-related metrics. In particular, we aim at verifying if the details provided by the app store could be leveraged for improving the results achieved by approaches exclusively based on code-related information.
In particular, as third-party libraries often represent carriers for app vulnerabilities , we used LibRadar  to extract the number of third-party libraries used by each application in our dataset. In addition, we used the apk-parser tool 7 for extracting the following features from each apk files: (i) the minimum API Level required for the application to run (min sdk), (ii) the specific API Level that the application targets (target sdk), (iii) the number of classes (classes), (iv) the number of packages (packages), (v) the number of interfaces (interfaces), (vi) the number of annotated classes (annotations), and (vii) the number of public classes (public classes). While the numbers of classes of different types determine the size of an app, which plays an important role in determining the security of the app (i.e., more classes mean a larger attack surface (Alenezi and Almomani 2018)), the API Levels indicators are strictly connected with the app security .
# Grouping of features
For convenience, the different extracted and pre-processed features have been grouped as follows:
- Market metrics: comprising app market features without considering the app Description, the app Name and the resulting textual features.
- Textual features: comprising all the words and n-grams derived from the text processing steps described above.
- Static analysis metrics: comprising all the code related metrics (i.e., libraries, min sdk, target sdk, classes, interfaces, annotations, public classes)
# Training of the ML models
We trained the ML algorithms using different combinations of these groups of features, namely (i) Market metrics + Textual features, (ii) Market metrics, (iii) Static analysis metrics, (iv) Market metrics + Textual features + Static analysis metrics, and (v) Market metrics + Static analysis metrics. As the goal of RQ3 is to understand the extent to which contextual information provided in the app store is useful (or not) for estimating app vulnerability-proneness, note that we only considered the combinations of features involving the Market metrics and a baseline (i.e., Static analysis metrics) for comparison purposes.
To alleviate concerns related to overfitting and selection bias, all the ML experiments were carried-out by using the 10-fold cross-validation strategy, while the classification/prediction performance achieved by the different ML models were evaluated through widely-known metrics in the information retrieval field: precision, recall and F-measure (Baeza-Yates et al. 1999).
# 4 Results
This section discusses the results of RQ1-3 (see Section 3).
# 4 RQ 1 : Which are the different vulnerabilities exhibited by Google market apps belonging to different app categories?
All the considered apps exhibit at least one vulnerability warning. The high numbers of potential vulnerabilities identified were quite expected, as vulnerability warnings considered in our study are of different severity levels, and some of them (the ones with lower severity levels) could be deliberately neglected by developers . Mobile apps significantly differ from more traditional software systems (Minelli and Lanza 2013b), as Android apps are poorly tested and also test cases have low code coverage . This could depend on the fact that the majority of app developers do not use test automation frameworks  and practices , while security testing is often neglected . Besides, app developers pose more attention to functional bugs and security bugs get fixed slower , also because fixing vulnerabilities requires specialized knowledge that is not widespread among developers .
As illustrated in Fig. 1, apps in the Medical category are the ones exhibiting the lowest level of vulnerability-proneness. Since attackers often make use of automated tools to identify likely exploitable attack vectors , we can observe that Medical apps
30
25
20
15
10
8
5
are less risky than other types of apps, thanks to a less wide variety of warnings to known vulnerabilities. More specifically, the differences in terms of vulnerability-proneness levels between the different app categories are statistically significant (the Kruskal-Wallis test returned p = 8 ∗ 10−5). The subsequent Mann-Whitney pairwise comparisons highlighted that the vulnerability-proneness of apps in the Medical category significantly differs from apps in the Communication, Entertainment, Food & Drink, News & Magazines, and Social categories with large effect-size. From these first results, we could observe that, on a positive side, there seems to be special attention towards security and privacy concerns in the development of apps in the Medical category, that often deal with very sensitive information (e.g. disease status, medications, etc.). However, from the security and privacy perspective, the same attention can not be noticed in the development of apps of the Finance or Shopping categories, which can also handle very sensitive information like bank accounts (e.g. the Finance category comprises several mobile banking apps).
To have a clearer picture of the potential security defects occurring in the investigated app categories, for brevity’s sake, in the following, we focus our analysis on the categories whose vulnerability-proneness levels exhibited statistically significant differences with large effect size. In particular, in Table 3, for the most recurrent vulnerability warnings marked as Critical (on the rows) and the app categories exhibiting statistically significant differences in the vulnerability-proneness levels (on the columns), the percentages of likely vulnerable apps are reported. Comparing such percentages, we found that:
- 938 out of 1002 (i.e., 93%) total apps in our dataset connect to URLs that are not under SSL, thus such communications are insecure by construction. In particular, 100% of considered apps belonging to the Communication, Food & Drink, and News & Magazines categories suffer from this vulnerability warning that is quite common also in apps handling very sensitive data, as those belonging to the Medical category (i.e., 84% of apps in this category exhibit this vulnerability).
- 692 (i.e., 69%) apps in our dataset make use of the addJavascriptInterface method, that can be used to allow JavaScript to control the application, in devices running older Android versions. More specifically, more than 80% of apps belonging to the Entertainment, Food & Drink, and News & Magazines categories suffer from this vulnerability, while only 36% of apps in the Medical category exhibited this security flaw. We conjecture that such diversified results are strongly connected with the actual testing practices that mobile developers perform on the considered apps. Indeed, as reported in previous work (Minelli and Lanza 2013a), mobile apps tend to have less (and in some cases no) test cases compared to other types of applications, despite their potential higher level of popularity among users. It is worth noticing that, as AndroBugs detects the minSdk (i.e., the minimum API Level on which the application is able to run), it only reports this vulnerability as critical when the addJavascriptInterface method can likely execute.
- 420 (i.e., 41%) of considered apps use implicit intents to start services, which is very risky because the responding service can not be identified. While this vulnerability is rare in apps of the Medical category (i.e., 15% of apps in this category are vulnerable), it has been found to be quite frequent in apps falling in the Social category (i.e., 54% of Social apps in our dataset suffer from this vulnerability).
- 279 (i.e., 27%) of considered apps perform insecure data storage by creating world-readable or world-writeable files. More than 30% of apps belonging to Food & Drink and News & Magazines categories exhibit this vulnerability.
# Empir Software Eng (2021) 26: 78
– 201 (i.e., 20%) of considered apps do not check the validity of SSL Certificate, allowing self-signed, expired or mismatch CN certificates for SSL connection. 40% of News & Magazines apps considered in our dataset result vulnerable to this flaw, while in only 6% of Medical apps we encountered this security defect.
– 200 (i.e., 19%) of considered apps do not protect KeyStore properly as they seem to use byte array or hard-coded certificate info to do SSL pinning. 35% of News & Magazines apps encompassed in our dataset exhibit this security hole, while only about 9% of Medical apps result vulnerable to this flaw.
– 191 (i.e., 19%) of considered apps use the critical function Runtime.getRuntime().exec(...), allowing attackers to inject arbitrary system commands. 34% of Communication apps and 31% of News & Magazines apps in our dataset exhibit this vulnerability, while it has been found in less than 13% of apps belonging to the Social and Medical categories.
– 158 (i.e., 15%) of considered apps have been found to be vulnerable to fragment injection, making it possible to access sensitive information that should not be accessible by the application itself on devices running older versions of Android. This vulnerability has been discovered in 22% of apps belonging to the Communication category while only about 3% of Medical apps present this security flaw.
– 156 (i.e., 15%) of considered apps use exported Content providers, making it possible to any other app on the device to access it. More than 24% of apps belonging to the News & Magazines category suffer from this type of vulnerability, while less than 10% of apps belonging to the Medical category exhibited this security flaw.
– 148 (i.e., 14%) of considered apps allow Self-defined HOSTNAME VERIFIER to accept all Common Names(CN), making it possible that malicious users perform Man-In-The-Middle (MITM) attacks. In particular, about 30% of apps belonging to the News & Magazines category exhibit this vulnerability.
– 114 (i.e., 11%) of considered apps do not check the validation of the CN(Common Name) of the SSL certificate and this could allow attackers to perform MITM attacks. More than 26% of the apps belonging to the News & Magazines category suffer from this vulnerability.
In summary, apps in the News & Magazines category tend to be more exposed than the other apps to vulnerabilities that could be likely exploited for performing Man-In-the-Middle attacks (i.e., SSL Connection Checking, SSL Certificate Verification Checking, and SSL Implementation Checking). In addition, News & Magazines apps tend to be quite prone also to vulnerabilities (i) that may cause injection attacks (i.e., vulnerabilities related to WebView and Runtime command), and (ii) that may allow unauthorized access to sensitive data (i.e., KeyStore protection vulnerability). It is worth noticing that also apps in the Communication category are (i) more prone than others to vulnerability warnings related to injection attacks (i.e., Runtime command, and Fragment vulnerabilities), and (ii) make more frequent use of implicit Intents, as apps in the Social category. Finally, the most recurrent vulnerability warnings found in the applications of the Medical category are related to the connection to URLs that are not under SSL, and the usage of vulnerable WebView, while any other critical warning has been found in less than 16% of apps belonging to this category.
Surprisingly, looking at Fig. 2 we can easily observe that, while Medical apps exhibit the lowest levels of vulnerability-proneness (probably due to the reduced sizes of this kind of apps that decrease the probability of releasing vulnerable code) apps in this category are also the ones exhibiting the highest vulnerability-proneness density. Indeed, the differences
# Empir Software Eng (2021) 26: 78
# 4 RQ 2 : Does the vulnerability-proneness of Google market apps affect app success?
As illustrated in Fig. 3, in terms of vulnerability-proneness levels, no significant differences could be observed between apps having different rating scores. Indeed, the Kruskal-Wallis test returned p = 0.
To investigate more in-depth whether users of apps in specific categories could be more concerned about security and privacy aspects, as stated in Section 3, we repeated the analysis for each specific app category. The results of such an analysis partially confirm our conjecture that the vulnerability-proneness of an app does not generally affect user ratings. Indeed, for none of the considered categories, we observed statistically significant differences between the vulnerability-proneness levels of apps with different ratings.
0
in terms of vulnerability-proneness density between the different app categories are statis-
tically significant (the Kruskal-Wallis test returned p = 1 ∗ 10−6). Specifically, the Medical apps significantly differ from apps in the Entertainment, Social, and Dating categories with large effect size, while Weather apps significantly differ from apps in the Social and Entertainment category with medium effect size. We can conclude that although Medical apps exhibit the highest vulnerability-proneness density levels, they tend to be less prone to potentially critical vulnerabilities than other kinds of apps, as evidenced in our qualitative analysis.
# Empir Software Eng (2021) 26: 78
Page 19 of 31
# 8
last result was quite unexpected, as some of the studied categories encompass apps handling very sensitive data (e.g. Medical, Finance, Shopping, etc.).
As done in RQ 1, we also conducted a normalized analysis to better understand if any relations can be observed between vulnerability-proneness density and the app rating. As shown in Fig. 4, apps having a lower average rating tend to have a higher vulnerability-proneness density. More specifically, the Kruskal-Wallis test highlighted that the differences in terms of vulnerability-proneness density among apps having different rating are statistically significant (i.e., the Kruskal-Wallis test returned p = 1 ∗ 10−5), while the post-hoc Mann-Whitney pairwise comparisons revealed that the differences are statistically significant for the following pairs: (i) apps with r < 3 and apps with r > 4, (ii) apps with
r <= 3
3 < r <= 4
4 < r <= 4
r > 4
Distributions of detected vulnerabilities in apps belonging to different rating groups
Distributions of vulnerability-proneness density in apps belonging to different rating groups
r &lt; 3 and apps with 4 &lt; r &le; 4, (ii) apps with 3 &lt; r &le; 4 and apps with r &gt; 4, and (iv) apps with 3 &lt; r &le; 4 and apps with 4 &lt; r &le; 4. However, such differences exhibit negligible to small effect sizes (i.e., Cliff’s d ranges from 0 to 0).
Though no significant relationships could be found between the vulnerability-proneness levels of apps and app ratings, differences with negligible or small effect sizes can be observed between the vulnerability-proneness density of apps with different ratings. Thus, we can argue that a higher vulnerability-proneness density may lead users to give lower ratings.
For better understanding if security and privacy concerns are taken into consideration by users when selecting apps to install, as reported in Section 3, we also measured the vulnerability-proneness levels of apps having different number of installs. As shown in Fig. 5, more popular apps tend to exhibit higher levels of vulnerability-proneness. More specifically, the Kruskal-Wallis test returned p = 1 ∗ 10−8 and the posthoc Mann-Whitney pairwise comparisons revealed that the vulnerability-proneness levels of apps belonging to the up to 1M group are different with statistical evidence from those of apps belonging to both up to 50M and more than 50M groups with small and medium effect-sizes, respectively.
However, this result could be due to the fact that more popular apps likely have larger sizes (Corral and Fronza 2015). Indeed, more meaningful differences are observed when looking at the vulnerability-proneness density of the apps with different levels of installs, as shown in Fig. 6. In this case, the Kruskal-Wallis test returned p &lt; 2 ∗ 10−16, while the post-hoc Mann-Whitney pairwise comparisons revealed that the differences are statistically significant for all the pairs with medium to large effect sizes (i.e., Cliff’s d varies from 0 to 0).
The conjunction of these results leads us to conclude that, while the vulnerability-proneness of apps does not seem to significantly impact their success, more popular apps tend to exhibit a lower vulnerability-proneness density. Despite app markets do not provide enough information to help users carefully selecting higher quality apps (Canfora et al.
# 3
# 8
# 3
# 8
|
2016; Di Sorbo et al. 2021), Google Play Protect8 can lead users more towards installing apps with lower vulnerability-proneness density. Besides, the lower vulnerability-proneness density of apps with higher ratings and installs could also depend on the fact that this kind of apps is usually developed (and maintained) by more experienced developers , also considering that larger apps tend to be tested more .
We argue that, for achieving the adoption of higher security standards in app development, the app markets could also provide details about the security risks to which users would be exposed, along with the information about an app’s functionalities. In this context, to promote their apps, developers would be forced to avoid known vulnerabilities and common security flaws, as user choices could also depend on the security aspects.
# 4 RQ 3 : Is it possible to predict the level of vulnerability-proneness of an app by using the app’s contextual information?
For brevity’s sake, in the following, we focus on
8 https://www.android.com/safety/
Precision (P), Recall (R) and F-measure (F1) obtained by the ML algorithms trained with the different combinations of features
Discussing the results achieved by the best performing model, i.e., Random Forest, in the various experiments.
On the one hand, by only using app market information (i.e., Market metrics in Table 4), the Random Forest algorithm achieved precision, recall and F-measure values of about 73%. This means that in about 3 out of 4 cases the classification algorithm based on these features can properly identify apps with either low or high vulnerability-proneness levels. This is a promising result if we consider that when the same algorithm is trained by using code-related features (i.e., Static analysis metrics in Table 4) slightly lower performance is achieved. On the other hand, the information provided by Google Play is not sufficient to make very accurate predictions. For this reason, the app markets should provide additional information useful to warn users of possible security and privacy risks, with the explicit aim of improving their awareness about security concerns.
Interestingly, when both app market information and code-related features are jointly used (i.e., Market metrics + Static analysis metrics in Table 4), higher precision, recall, and F-measure values are obtained. This means that app market metrics provide complementary information to the one related to code. Therefore, they could be adopted for improving strategies exclusively based on code-related characteristics. Surprisingly, textual features seem to introduce noise that degrades the classification results. Indeed, in both experiments in which textual contents have been employed (i.e., Market metrics + Textual features and Market metrics + Textual features + Static analysis metrics in Table 4) lower performance is achieved. We argue that this result is mainly due to the fact that topics discussed in app name and description is not strictly related to the vulnerability proneness of an app.
To more-in-depth understand the specific app market information that could drive the classification, we computed the information gain  for each of the features in the Market metrics group and ranked these features based on their scores. In Table 5, the top 15 ranked features, along with the related information gain scores are reported. Looking at Table 5, we can observe that the app size is the feature with the highest score. Besides, permissions-related aspects such as (i) the number of permission, and (ii) the presence of specific permissions (e.g. mostly related to storage and networking operations) required by the app represent relevant information for deciding the vulnerability-proneness level of an app. Finally, as further confirmation of what we found in RQ 1 (Section 4) and RQ 2 (Section 4), both the Category of an app and its popularity indicators (i.e., Raters and ...
# Empir Software Eng (2021) 26: 78
Installs) provide valuable information for identifying the risk levels. Since the Size feature represents the best predictor, the resulting model might be biased towards this metric (i.e., the predictions of vulnerability-proneness levels could be mainly based on this feature). Thus, to estimate the extent to which the performance of the ML models is driven by the Size metric, we repeated the ML experiments (in which ML algorithms are fed with Market metrics) by not considering this feature and observed degradations in precision, recall, and F-measure values lower than 5%. This allows us to conclude that the eventual bias introduced by the Size metric is only marginal.
# 5 Threats to Validity
Threats to construct validity concern the relationship between theory and observation. The most important threat that could affect the results of our study is related to possible imprecision/incompleteness in identifying the vulnerabilities and, thus, the vulnerability-proneness levels of apps. It is worth noticing that we rely on a state-of-the-art tool (i.e., AndroBugs) for identifying well-known security flaws. As this is a static analysis tool, it may yield false positives. In addition, in our RQ2 we used the average rating as an indicator of the success of an app. However, user ratings can be subjective and imprecise. To alleviate such an issue, we (i) only considered apps rated by a reliable number of users (i.e., Raters > 500), and (ii) complemented the analysis by also considering the number of installs as an alternative way.
to measure app success. We believe that, with a smaller number of ratings, there is a higher risk that our results may depend on the subjectiveness of the ratings. Indeed, considering only a few tens of ratings, extremely positive or negative ratings given by certain groups of users may have too much impact on the average score . For this reason, the average rating value of an app that is rated by at least 500 users is more likely to take into account the opinions of more heterogeneous users and, thus, better reflects the actual rating. Our study does not consider the temporal dimension associated with the user rating and the number of installs, and this could represent a threat that can affect the validity of our findings. Unfortunately, Google Play does not allow extracting the user ratings and number of installs referring to a specific app version, and only average or cumulative counts are provided. However, prior research  demonstrated that average user rating is quite resilient to version-rating changes, especially for apps having higher numbers of raters. Finally, our measurements on vulnerability proneness are based on the assumption that users need to be aware of the security level of all apps. However, the usage of specific vulnerability-prone apps could be unavoidable for users when real alternatives are not available to replace popular apps (e.g., Facebook, Whatsapp, etc.). This means that it is unclear weather users having more information about the level of vulnerabilities of popular apps can push them to not install or use them. This is something we plan to investigate for future work.
# Threats to conclusion validity
Threats to conclusion validity concern the relationship between treatment and outcome. Appropriate, non-parametric statistical procedures have been adopted to draw our conclusions, since the variables of interest were not well-modeled by normal distributions (as verified through the Shapiro-Wilk normality test (Shapiro and Wilk 1965)). More specifically, we used the Kruskal-Wallis test and post hoc Mann-Whitney pairwise comparisons for investigating the statistically significant differences. Moreover, the magnitude of the observed differences is quantified by using Cliff’s delta effect size measure. For coping with multiple comparisons, the Holm’s correction procedure has been adopted to adjust p-values.
# Threats to internal validity
Threats to internal validity concern factors that can affect our results. A possible source of bias might be related to the thresholds we used when analyzing the data and presenting our results. In particular, to answer our RQ2, we clustered the apps into four levels of rating and three levels of installs. Similarly, to answer our RQ3, we grouped the apps into two vulnerability-proneness level groups. The thresholds to define the average rating categories and the vulnerability-proneness categories were based on the descriptive statistics indicators (i.e., quartiles) of the related distributions for the 1,002 considered apps, while the apps have been assigned to the different levels of installs based on the downloads category assigned by the Google Play store (see Section 3). Different choices might lead to different results. Nevertheless, we obtained similar results when considering different (i) rating —e.g. low, encompassing the apps with an average rating lower than the first quartile, medium comprising the apps with an average rating value between the first and the third quartile, and high, consisting of the apps with an average rating higher than the third quartile– and (ii) downloads groups. To verify weather the adopted independent features (or variables) could correlate among themselves we computed the Kendall correlation between all the pairs of (numeric) features considered in our study. As result, no correlations with either large or medium effect size are observed (τB ≤ 0 for all the pairs of features).
# Threats to external validity
Threats to external validity concern the generalization of the findings. Our analysis involves about 1,000 Android apps sampled from a dataset not specifically designed for security purposes. Thus, it is unclear if our results may generalize to further apps or apps developed for other mobile platforms. However, as discussed in Section 3, our data collection is a
statistically significant sample of a dataset consisting of about 10,000 apps. While our results are in line with previous investigations demonstrating that also very popular apps suffer from insecure communication vulnerabilities (Gajrani et al. 2020; Papageorgiou et al. 2018; Taylor and Martinovic 2017b), it is worth pointing out that some of these vulnerabilities could be non-exploitable (i.e., appearing in dead or legacy code), or present in third-party libraries . To estimate the trustworthiness of our results, as well as the actual exploitability of the signaled vulnerabilities, we randomly selected 20 apps in our dataset. We reverse-engineered (by using dex2jar9) such apps and inspected their source code to understand whether the vulnerable code portions reported by AndroBugs and marked as Critical were actually reachable. As a result of this analysis, we observed that less than 5% of the signaled critical vulnerabilities appeared in (statically detectable) unreachable code. Besides, we performed a dynamic analysis on the same 20 selected apps, following the approach used in previous work (Darvish and Husain 2018). Indeed, to conduct such an analysis, we used the Charles Proxy10 and tried to perform man-in-the-middle (MITM) attacks, by testing the SSL certificate validation, SSL certificate checking, and that all important URLs are SSL protected. Specifically, we opened each target application and tried different functionalities of the application while intercepting the packets on the Charles proxy. The attack succeeded if we were able to read (or decode) the intercepted packets. Despite for all the 20 analyzed apps AndroBugs reported issues with SSL certificates, for four of them (i.e., 20%), we were unable to trigger the attack. Furthermore, we only considered free apps and it is unclear if our conclusions are also valid for paid apps, as more rigorous processes for avoiding security flaws could be adopted when developing paid apps. To further increase the generalizability and reliability of our results, in the future, we plan to replicate our study at a larger scale considering apps from other datasets  and investigating if our findings are still valid when considering paid apps.
# 6 Conclusions and Future Work
Users typically share sensitive information to use mobile apps for accomplishing a lot of everyday life activities. However, recent research demonstrated that the majority of these apps suffer from critical security defects.
In this paper, we defined the concept of vulnerability-proneness of an app and investigated if different vulnerability warnings and vulnerability-proneness levels are observed in apps belonging to different Google Play’s categories (RQ 1). Moreover, we also explored the extent to which vulnerability-proneness of mobile apps can affect the overall app success, measured in terms of average app rating and number of app downloads (RQ2). Finally, we proposed to use app contextual information to predict the vulnerability-proneness level of an app (RQ3).
The results of our study demonstrated that most of the considered apps exhibit at least one critical vulnerability warning. On a positive side, (i) apps in the Medical category, that usually handle very sensitive information, tend to be less prone to potentially critical vulnerabilities, while (ii) there is not the same attention on security warnings in apps belonging to other categories that also deal with sensitive data (e.g., Finance, Shopping, etc.). Our work also proved that, while no strong relations could be observed between the
9 https://github.com/pxb1988/dex2jar
10 https://www.charlesproxy.com/
# Empir Software Eng (2021) 26: 78
vulnerability-proneness of an app and its average rating, apps with a higher number of downloads tend to have higher vulnerability-proneness levels, but a lower vulnerability-proneness density. Finally, we also showed how an app’s contextual information can be used to predict, in the early stages, its vulnerability-proneness level.
Our work can have important implications for app markets, developers and users. Indeed, on the one hand, the proposed classification could help users selecting apps that exhibit lower risk levels. On the other hand, the app markets could integrate approaches similar to ours, to provide timely and relevant information about security and privacy risks to users before installing an app. We believe that the usage of such mechanisms can stimulate developers (interested in promoting their apps) to apply the best security practices and carefully avoid known security flaws.
As future work, we plan to study the vulnerability-proneness of apps developed for different platforms (e.g., iOS). In this context, it could be interesting to investigate if the same apps exhibit different vulnerability-proneness levels when considering different versions (e.g. Android vs. iOS). Moreover, further information from the app store (e.g. interactive elements, developer-related information, user review comments, etc.)  could be collected to improve the classification results. Very important, as future work, is also to empirically investigate the extent to which users are unaware of the level of vulnerability-proneness of apps as well as the extent to which our approach can actually help them understanding more about the potential risks of downloading or using an app. In this context, we plan to (i) leverage summarization  techniques to generate reports on the vulnerability-proneness levels of mobile apps, and (ii) use such reports to support users in making decisions on downloading (or using) specific apps. Moreover, we plan to explore the extent to which an app’s contextual information is useful to predict the presence or absence of specific security flaws.
# Empir Software Eng (2021) 26: 78
Felt AP, Wang HJ, Moshchuk A, Hanna S, Chin E (2011b) Permission re-delegation: Attacks and defenses. In: USENIX security symposium
Gajrani J, Tripathi M, Laxmi V, Somani G, Zemmari A, Gaur MS (2020) Vulvet: Vetting of vulnerabilities in android apps to thwart exploitation. Digit Threats Res Practice 1(2):1–25
Gao J, Li L, Kong P, Bissyand´e TF, Klein J (2019) Understanding the evolution of android app vulnerabilities. IEEE Trans Reliab:1–19. https://doi.org/10/TR
Gartner (2015) Gartner Says More than 75 Percent of Mobile Applications will Fail Basic Security Tests Through 2015. https://tinyurl.com/uavh5nq. Online; accessed 20 January 2020
Giger E, D’Ambros M, Pinzger M, Gall HC (2012) Method-level bug prediction. In: International Symposium on Empirical Software Engineering and Measurement, pp 171–180
Gorla A, Tavecchia I, Gross F, Zeller A (2014) Checking app behavior against app descriptions. In: International Conference on Software Engineering, pp 1025–1035
Grano G, Di Sorbo A, Mercaldo F, Visaggio CA, Canfora G, Panichella S (2017) Android apps and user feedback: a dataset for software evolution and quality improvement. In: Proceedings of the 2nd ACM SIGSOFT International Workshop on App Market Analytics, WAMA@ESEC/SIGSOFT FSE 2017, Paderborn, pp 8–11
Grissom RJ, Kim JJ (2005) Effect sizes for research: A broad practical approach, 2nd edn. Lawrence Earlbaum Associates
Guerrouj L, Azad S, Rigby PC (2015) The influence of app churn on app success and stackoverflow discussions. In: International Conference on Software Analysis, Evolution, and Reengineering, pp 321–330
Harman M, Jia Y, Zhang Y (2012) App store mining and analysis: MSR for app stores. In: Working Conference of Mining Software Repositories, pp 108–111
Hay R, Tripp O, Pistoia M (2015) Dynamic detection of inter-application communication vulnerabilities in android. In: International Symposium on Software Testing and Analysis, pp 118–128
Holm S (1979) A simple sequentially rejective multiple test procedure. Scand J Stat 6(2):65–70
Islam MR (2014) Numeric rating of apps on google play store by sentiment analysis on user reviews. In: International Conference on Electrical Engineering and Information & Communication Technology. IEEE, pp 1–4
Jimenez M, Papadakis M, Bissyand´e TF, Klein J (2016) Profiling android vulnerabilities. In: International Conference on Software Quality, Reliability and Security, pp 222–229
Johann T, Stanik C, B. AMA, Maalej W (2017) SAFE: A simple approach for feature extraction from app descriptions and app reviews. In: International Requirements Engineering Conference, pp 21–30
Kallis R, Di Sorbo A, Canfora G, Panichella S (2019) Ticket tagger: Machine learning driven issue classification. In: 2019 IEEE International Conference on Software Maintenance and Evolution, pp 406–409
Kantola D, Chin E, He W, Wagner DA (2012) Reducing attack surfaces for intra-application communication in android. In: Workshop on Security and Privacy in Smartphones and Mobile Devices, Co-located with CCS 2012, pp 69–80
Kaur A, Kaur I (2014) Empirical evaluation of machine learning algorithms for fault prediction. Lect Notes Softw Eng 2(2):176
Khalid H, Nagappan M, Hassan AE (2016) Examining the relationship between findbugs warnings and app ratings. IEEE Softw 33(4):34–39. https://doi.org/10/MS
Kochhar PS, Thung F, Nagappan N, Zimmermann T, Lo D (2015) Understanding the test automation culture of app developers. In: 8th IEEE International Conference on Software Testing, Verification and Validation, ICST 2015, Graz, Austria, April 13-17, 2015, pp 1–10
Kruskal WH, Wallis WA (1952) Use of ranks in one-criterion variance analysis. J Amer Stat Assocss 47(260):583–621
Krutz DE, Munaiah N, Meneely A, Malachowsky SA (2016) Examining the relationship between security metrics and user ratings of mobile apps: a case study. In: Proceedings of the International Workshop on App Market Analytics, pp 8–14
Li L, Bartel A, Bissyand´e TF, Klein J, Le Traon Y, Arzt S, Rasthofer S, Bodden E, Octeau D, McDaniel P (2015) Iccta: Detecting inter-component privacy leaks in android apps. In: IEEE International Conference on Software Engineering, vol 1, pp 280–291
Lu L, Li Z, Wu Z, Lee W, Jiang G (2012) CHEX: statically vetting android apps for component hijacking vulnerabilities. In: the ACM Conference on Computer and Communications Security, pp 229–240
Lyu Y, Gui J, Wan M, Halfond WGJ (2017) An empirical study of local database usage in android applications. In: 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017, Shanghai, China, September 17-22, 2017, pp 444–455
# Empir Software Eng (2021) 26: 78
# Empir Software Eng (2021) 26: 78
Page 31 of 31  78
# Publisher’s note
Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
# Author Information
Andrea Di Sorbo is a research fellow at the University of Sannio, Italy. He received the PhD degree in information technology from the University of Sannio, in 2018. His research interests include software maintenance and evolution, mining software repositories, empirical software engineering, text analysis and software security and privacy. He authored (or co-authored) several papers appeared in flagship international conferences (ICSE, FSE, ASE, ICSME) and journals (TSE, JSS, IST, JSEP). He serves and has served as review editor and guest associate editor for Frontiers in Big Data, guest editor for the Information and Software Technology journal, and reviewer for several journals in the field of software engineering, such as Transactions on Software Engineering, edited by IEEE, Transactions on Software Engineering and Methodology, edited by ACM, and the Empirical Software Engineering journal edited by Springer. He is also a program committee member of some international conferences (ARES, MOBILESoft, SEAA).
Sebastiano Panichella is a Computer Science Researcher at Zurich University of Applied Science (ZHAW). His main research goal is to conduct industrial research, involving both industrial and academic collaborations, to sustain the Internet of Things (IoT) vision. His research interests are in the domain of Software Engineering (SE), cloud computing (CC), and Data Science (DS): DevOps (e.g., Continuous Delivery, Continuous integration), Machine learning applied to SE, Software maintenance and evolution (with particular focus on Cloud, mobile, and Cyber-physical applications), Mobile Computing. Moreover, he is promoting DS research on “Summarization Techniques for Code, Changes, and Testing”. Currently he is technical coordinator of H2020 and Innosuisse projects concerning DevOps for Complex Cyber-physical Systems. He authored or co-authored more than seventy papers appeared in International Conferences and Journals. These research works involved studies with industrial and open projects and received best paper awards or best paper nominations. He serves and has served as program committee member of various international conference and as reviewer for various international journals in the fields of software engineering. In 2017, he was selected as one of the top-20 Most Active Early Stage Researchers Worldwide in SE. Home page: https://spanichella.github.io.